{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gz6CwD6MRWFs"
   },
   "source": [
    "##Install Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nD9uOPEK65K0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9AwVn7yc7Soj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'MedViT' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/MedViT'\n",
      "/Users/maggieyao/computer_vision/cv-final-project/CVFinalProject\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Omid-Nejati/MedViT.git\n",
    "%cd /content/MedViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RPHDM3KwQ4Ys"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: timm in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 2)) (0.9.16)\n",
      "Requirement already satisfied: medmnist in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 3)) (3.0.1)\n",
      "Requirement already satisfied: numpy in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 5)) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: scikit-image in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 7)) (0.23.2)\n",
      "Requirement already satisfied: fvcore in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 8)) (0.1.5.post20221221)\n",
      "Requirement already satisfied: tqdm in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 9)) (4.66.2)\n",
      "Requirement already satisfied: Pillow in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 10)) (10.3.0)\n",
      "Requirement already satisfied: fire in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: torchattacks in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 12)) (3.5.1)\n",
      "Requirement already satisfied: torch in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 13)) (2.3.0)\n",
      "Requirement already satisfied: torchvision in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from -r requirements.txt (line 14)) (0.18.0)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from timm->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from timm->-r requirements.txt (line 2)) (0.17.3)\n",
      "Requirement already satisfied: safetensors in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from timm->-r requirements.txt (line 2)) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from pandas->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from pandas->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from pandas->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.4.0)\n",
      "Requirement already satisfied: networkx>=2.8 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from scikit-image->-r requirements.txt (line 7)) (3.3)\n",
      "Requirement already satisfied: imageio>=2.33 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from scikit-image->-r requirements.txt (line 7)) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from scikit-image->-r requirements.txt (line 7)) (2024.4.24)\n",
      "Requirement already satisfied: packaging>=21 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from scikit-image->-r requirements.txt (line 7)) (24.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from scikit-image->-r requirements.txt (line 7)) (0.4)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from fvcore->-r requirements.txt (line 8)) (0.1.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from fvcore->-r requirements.txt (line 8)) (2.4.0)\n",
      "Requirement already satisfied: tabulate in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fvcore->-r requirements.txt (line 8)) (0.9.0)\n",
      "Requirement already satisfied: iopath>=0.1.7 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from fvcore->-r requirements.txt (line 8)) (0.1.10)\n",
      "Requirement already satisfied: six in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from fire->-r requirements.txt (line 11)) (1.16.0)\n",
      "Requirement already satisfied: requests~=2.25.1 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from torchattacks->-r requirements.txt (line 12)) (2.25.1)\n",
      "Requirement already satisfied: filelock in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from torch->-r requirements.txt (line 13)) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from torch->-r requirements.txt (line 13)) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from torch->-r requirements.txt (line 13)) (1.12)\n",
      "Requirement already satisfied: jinja2 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from torch->-r requirements.txt (line 13)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from torch->-r requirements.txt (line 13)) (2024.3.1)\n",
      "Requirement already satisfied: portalocker in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from iopath>=0.1.7->fvcore->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from requests~=2.25.1->torchattacks->-r requirements.txt (line 12)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from requests~=2.25.1->torchattacks->-r requirements.txt (line 12)) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests~=2.25.1->torchattacks->-r requirements.txt (line 12)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests~=2.25.1->torchattacks->-r requirements.txt (line 12)) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from jinja2->torch->-r requirements.txt (line 13)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/maggieyao/Library/Python/3.10/lib/python/site-packages (from sympy->torch->-r requirements.txt (line 13)) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lgm5vmQp8i9h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.utils\n",
    "from torchvision import models\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "import torchattacks\n",
    "from torchattacks import PGD, FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "deLoJPfDTDWl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 2.3.0\n",
      "Torchvision 0.18.0\n",
      "Torchattacks 3.5.1\n",
      "Numpy 1.26.4\n",
      "Medmnist 3.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch\", torch.__version__)\n",
    "print(\"Torchvision\", torchvision.__version__)\n",
    "print(\"Torchattacks\", torchattacks.__version__)\n",
    "print(\"Numpy\", np.__version__)\n",
    "print(\"Medmnist\", medmnist.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medmnist 3.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Medmnist\", medmnist.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIefIFDW80-U"
   },
   "source": [
    "##Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVNzCWaVUd_1"
   },
   "source": [
    "data_flag =  \n",
    "[tissuemnist, pathmnist, chestmnist, dermamnist, octmnist, pnemoniamnist, retinamnist, breastmnist, bloodmnist, tissuemnist, organamnist, organcmnist, organsmnist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "rH1INOxS8-iM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of channels :  3\n",
      "number of classes :  5\n"
     ]
    }
   ],
   "source": [
    "data_flag = 'retinamnist'\n",
    "# [tissuemnist, pathmnist, chestmnist, dermamnist, octmnist,\n",
    "# pnemoniamnist, retinamnist, breastmnist, bloodmnist, tissuemnist, organamnist, organcmnist, organsmnist]\n",
    "download = True\n",
    "\n",
    "# NUM_EPOCHS = 10\n",
    "# BATCH_SIZE = 10\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "# lr = 0.005\n",
    "lr = 0.003\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "print(\"number of channels : \", n_channels)\n",
    "print(\"number of classes : \", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "TD22o8uW9L1X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/maggieyao/.medmnist/retinamnist.npz\n",
      "Using downloaded and verified file: /Users/maggieyao/.medmnist/retinamnist.npz\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.transforms import Resize\n",
    "# preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.Lambda(lambda image: image.convert('RGB')),\n",
    "    torchvision.transforms.AugMix(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.Lambda(lambda image: image.convert('RGB')),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=train_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=test_transform, download=download)\n",
    "\n",
    "# pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "eNjqCTnI9T9w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset RetinaMNIST of size 28 (retinamnist)\n",
      "    Number of datapoints: 1080\n",
      "    Root location: /Users/maggieyao/.medmnist\n",
      "    Split: train\n",
      "    Task: ordinal-regression\n",
      "    Number of channels: 3\n",
      "    Meaning of labels: {'0': '0', '1': '1', '2': '2', '3': '3', '4': '4'}\n",
      "    Number of samples: {'train': 1080, 'val': 120, 'test': 400}\n",
      "    Description: The RetinaMNIST is based on the DeepDRiD challenge, which provides a dataset of 1,600 retina fundus images. The task is ordinal regression for 5-level grading of diabetic retinopathy severity. We split the source training set with a ratio of 9:1 into training and validation set, and use the source validation set as the test set. The source images of 3×1,736×1,824 are center-cropped and resized into 3×28×28.\n",
      "    License: CC BY 4.0\n",
      "===================\n",
      "Dataset RetinaMNIST of size 28 (retinamnist)\n",
      "    Number of datapoints: 400\n",
      "    Root location: /Users/maggieyao/.medmnist\n",
      "    Split: test\n",
      "    Task: ordinal-regression\n",
      "    Number of channels: 3\n",
      "    Meaning of labels: {'0': '0', '1': '1', '2': '2', '3': '3', '4': '4'}\n",
      "    Number of samples: {'train': 1080, 'val': 120, 'test': 400}\n",
      "    Description: The RetinaMNIST is based on the DeepDRiD challenge, which provides a dataset of 1,600 retina fundus images. The task is ordinal regression for 5-level grading of diabetic retinopathy severity. We split the source training set with a ratio of 9:1 into training and validation set, and use the source validation set as the test set. The source images of 3×1,736×1,824 are center-cropped and resized into 3×28×28.\n",
      "    License: CC BY 4.0\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(\"===================\")\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "# print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ta2wQYk78Mg"
   },
   "source": [
    "##Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a17pPgePWXga"
   },
   "source": [
    "MedViTs ---> [MedViT_small, MedViT_base, MedViT_large]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEQ5S3_U8E0j",
    "outputId": "0ea09038-9e57-42ab-b767-592098af81ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize_weights...\n"
     ]
    }
   ],
   "source": [
    "from MedViT import MedViT_small, MedViT_base, MedViT_large\n",
    "model = MedViT_small(num_classes = n_classes).cpu()\n",
    "#model = MedViT_base(num_classes = n_classes).cuda()\n",
    "#model = MedViT_large(num_classes = n_classes).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-ImKp2m9cLf"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "gMy_aJrE9eeM"
   },
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "if task == \"multi-label, binary-class\":\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "KmIo3JWf9lEs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/68 [00:23<26:45, 23.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti-label, binary-class\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     17\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/computer_vision/cv-final-project/CVFinalProject/MedViT.py:567\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      1\u001b[0m # \"\"\"\n\u001b[1;32m      2\u001b[0m # Author: Omid Nejati\n\u001b[1;32m      3\u001b[0m # Email: omid_nejaty@alumni.iust.ac.ir\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m # MedViT: A Robust Vision Transformer for Generalized Medical Image Classification.\n\u001b[1;32m      6\u001b[0m # \"\"\"\n\u001b[1;32m      7\u001b[0m # from functools import partial\n\u001b[1;32m      8\u001b[0m # import math\n\u001b[1;32m      9\u001b[0m # import torch\n\u001b[1;32m     10\u001b[0m # import torch.utils.checkpoint as checkpoint\n\u001b[1;32m     11\u001b[0m # from einops import rearrange\n\u001b[1;32m     12\u001b[0m # from timm.models.layers import DropPath, trunc_normal_\n\u001b[1;32m     13\u001b[0m # from timm.models.registry import register_model\n\u001b[1;32m     14\u001b[0m # from torch import nn\n\u001b[1;32m     15\u001b[0m # from utils import merge_pre_bn\n\u001b[1;32m     16\u001b[0m # import numpy as np\n\u001b[1;32m     17\u001b[0m # import torch\n\u001b[1;32m     18\u001b[0m # from torch import nn\n\u001b[1;32m     19\u001b[0m # from torch.nn import init\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m # NORM_EPS = 1e-5\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m # class ConvBNReLU(nn.Module):\n\u001b[1;32m     25\u001b[0m #     def __init__(\n\u001b[1;32m     26\u001b[0m #             self,\n\u001b[1;32m     27\u001b[0m #             in_channels,\n\u001b[1;32m     28\u001b[0m #             out_channels,\n\u001b[1;32m     29\u001b[0m #             kernel_size,\n\u001b[1;32m     30\u001b[0m #             stride,\n\u001b[1;32m     31\u001b[0m #             groups=1):\n\u001b[1;32m     32\u001b[0m #         super(ConvBNReLU, self).__init__()\n\u001b[1;32m     33\u001b[0m #         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n\u001b[1;32m     34\u001b[0m #                               padding=1, groups=groups, bias=False)\n\u001b[1;32m     35\u001b[0m #         self.norm = nn.BatchNorm2d(out_channels, eps=NORM_EPS)\n\u001b[1;32m     36\u001b[0m #         self.act = nn.ReLU(inplace=True)\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m #     def forward(self, x):\n\u001b[1;32m     39\u001b[0m #         x = self.conv(x)\n\u001b[1;32m     40\u001b[0m #         x = self.norm(x)\n\u001b[1;32m     41\u001b[0m #         x = self.act(x)\n\u001b[1;32m     42\u001b[0m #         return x\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m # def _make_divisible(v, divisor, min_value=None):\n\u001b[1;32m     46\u001b[0m #     if min_value is None:\n\u001b[1;32m     47\u001b[0m #         min_value = divisor\n\u001b[1;32m     48\u001b[0m #     new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n\u001b[1;32m     49\u001b[0m #     # Make sure that round down does not go down by more than 10%.\n\u001b[1;32m     50\u001b[0m #     if new_v < 0.9 * v:\n\u001b[1;32m     51\u001b[0m #         new_v += divisor\n\u001b[1;32m     52\u001b[0m #     return new_v\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m # class PatchEmbed(nn.Module):\n\u001b[1;32m     56\u001b[0m #     def __init__(self,\n\u001b[1;32m     57\u001b[0m #                  in_channels,\n\u001b[1;32m     58\u001b[0m #                  out_channels,\n\u001b[1;32m     59\u001b[0m #                  stride=1):\n\u001b[1;32m     60\u001b[0m #         super(PatchEmbed, self).__init__()\n\u001b[1;32m     61\u001b[0m #         norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n\u001b[1;32m     62\u001b[0m #         if stride == 2:\n\u001b[1;32m     63\u001b[0m #             self.avgpool = nn.AvgPool2d((2, 2), stride=2, ceil_mode=True, count_include_pad=False)\n\u001b[1;32m     64\u001b[0m #             self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n\u001b[1;32m     65\u001b[0m #             self.norm = norm_layer(out_channels)\n\u001b[1;32m     66\u001b[0m #         elif in_channels != out_channels:\n\u001b[1;32m     67\u001b[0m #             self.avgpool = nn.Identity()\n\u001b[1;32m     68\u001b[0m #             self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n\u001b[1;32m     69\u001b[0m #             self.norm = norm_layer(out_channels)\n\u001b[1;32m     70\u001b[0m #         else:\n\u001b[1;32m     71\u001b[0m #             self.avgpool = nn.Identity()\n\u001b[1;32m     72\u001b[0m #             self.conv = nn.Identity()\n\u001b[1;32m     73\u001b[0m #             self.norm = nn.Identity()\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m #     def forward(self, x):\n\u001b[1;32m     76\u001b[0m #         return self.norm(self.conv(self.avgpool(x)))\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m # # class MHCA(nn.Module):\n\u001b[1;32m     80\u001b[0m # #     \"\"\"\n\u001b[1;32m     81\u001b[0m # #     Multi-Head Convolutional Attention\n\u001b[1;32m     82\u001b[0m # #     \"\"\"\n\u001b[1;32m     83\u001b[0m # #     def __init__(self, out_channels, head_dim):\n\u001b[1;32m     84\u001b[0m # #         super(MHCA, self).__init__()\n\u001b[1;32m     85\u001b[0m # #         norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n\u001b[1;32m     86\u001b[0m # #         self.group_conv3x3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1,\n\u001b[1;32m     87\u001b[0m # #                                        padding=1, groups=out_channels // head_dim, bias=False)\n\u001b[1;32m     88\u001b[0m # #         self.norm = norm_layer(out_channels)\n\u001b[1;32m     89\u001b[0m # #         self.act = nn.ReLU(inplace=True)\n\u001b[1;32m     90\u001b[0m # #         self.projection = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=False)\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m # #     def forward(self, x):\n\u001b[1;32m     93\u001b[0m # #         out = self.group_conv3x3(x)\n\u001b[1;32m     94\u001b[0m # #         out = self.norm(out)\n\u001b[1;32m     95\u001b[0m # #         out = self.act(out)\n\u001b[1;32m     96\u001b[0m # #         out = self.projection(out)\n\u001b[1;32m     97\u001b[0m # #         return out\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m # class ChannelAttention(nn.Module):\n\u001b[1;32m    100\u001b[0m #     def __init__(self,channel,reduction=16):\n\u001b[1;32m    101\u001b[0m #         super().__init__()\n\u001b[1;32m    102\u001b[0m #         self.maxpool=nn.AdaptiveMaxPool2d(1)\n\u001b[1;32m    103\u001b[0m #         self.avgpool=nn.AdaptiveAvgPool2d(1)\n\u001b[1;32m    104\u001b[0m #         self.se=nn.Sequential(\n\u001b[1;32m    105\u001b[0m #             nn.Conv2d(channel,channel//reduction,1,bias=False),\n\u001b[1;32m    106\u001b[0m #             nn.ReLU(),\n\u001b[1;32m    107\u001b[0m #             nn.Conv2d(channel//reduction,channel,1,bias=False)\n\u001b[1;32m    108\u001b[0m #         )\n\u001b[1;32m    109\u001b[0m #         self.sigmoid=nn.Sigmoid()\n\u001b[1;32m    110\u001b[0m     \n\u001b[1;32m    111\u001b[0m #     def forward(self, x) :\n\u001b[1;32m    112\u001b[0m #         max_result=self.maxpool(x)\n\u001b[1;32m    113\u001b[0m #         avg_result=self.avgpool(x)\n\u001b[1;32m    114\u001b[0m #         max_out=self.se(max_result)\n\u001b[1;32m    115\u001b[0m #         avg_out=self.se(avg_result)\n\u001b[1;32m    116\u001b[0m #         output=self.sigmoid(max_out+avg_out)\n\u001b[1;32m    117\u001b[0m #         return output\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m # class SpatialAttention(nn.Module):\n\u001b[1;32m    120\u001b[0m #     def __init__(self,kernel_size=7):\n\u001b[1;32m    121\u001b[0m #         super().__init__()\n\u001b[1;32m    122\u001b[0m #         self.conv=nn.Conv2d(2,1,kernel_size=kernel_size,padding=kernel_size//2)\n\u001b[1;32m    123\u001b[0m #         self.sigmoid=nn.Sigmoid()\n\u001b[1;32m    124\u001b[0m     \n\u001b[1;32m    125\u001b[0m #     def forward(self, x) :\n\u001b[1;32m    126\u001b[0m #         max_result,_=torch.max(x,dim=1,keepdim=True)\n\u001b[1;32m    127\u001b[0m #         avg_result=torch.mean(x,dim=1,keepdim=True)\n\u001b[1;32m    128\u001b[0m #         result=torch.cat([max_result,avg_result],1)\n\u001b[1;32m    129\u001b[0m #         output=self.conv(result)\n\u001b[1;32m    130\u001b[0m #         output=self.sigmoid(output)\n\u001b[1;32m    131\u001b[0m #         return output\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m # class MHCA(nn.Module):\n\u001b[1;32m    136\u001b[0m #     \"\"\"\n\u001b[1;32m    137\u001b[0m #     Convolutional Block Attention Module\n\u001b[1;32m    138\u001b[0m #     \"\"\"\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m #     def __init__(self, out_channels, head_dim, reduction=16,kernel_size=49):\n\u001b[1;32m    141\u001b[0m #         super().__init__()\n\u001b[1;32m    142\u001b[0m #         self.ca=ChannelAttention(channel=out_channels,reduction=reduction)\n\u001b[1;32m    143\u001b[0m #         self.sa=SpatialAttention(kernel_size=kernel_size)\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m #     def init_weights(self):\n\u001b[1;32m    147\u001b[0m #         for m in self.modules():\n\u001b[1;32m    148\u001b[0m #             if isinstance(m, nn.Conv2d):\n\u001b[1;32m    149\u001b[0m #                 init.kaiming_normal_(m.weight, mode='fan_out')\n\u001b[1;32m    150\u001b[0m #                 if m.bias is not None:\n\u001b[1;32m    151\u001b[0m #                     init.constant_(m.bias, 0)\n\u001b[1;32m    152\u001b[0m #             elif isinstance(m, nn.BatchNorm2d):\n\u001b[1;32m    153\u001b[0m #                 init.constant_(m.weight, 1)\n\u001b[1;32m    154\u001b[0m #                 init.constant_(m.bias, 0)\n\u001b[1;32m    155\u001b[0m #             elif isinstance(m, nn.Linear):\n\u001b[1;32m    156\u001b[0m #                 init.normal_(m.weight, std=0.001)\n\u001b[1;32m    157\u001b[0m #                 if m.bias is not None:\n\u001b[1;32m    158\u001b[0m #                     init.constant_(m.bias, 0)\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m #     def forward(self, x):\n\u001b[1;32m    161\u001b[0m #         b, c, _, _ = x.size()\n\u001b[1;32m    162\u001b[0m #         residual=x\n\u001b[1;32m    163\u001b[0m #         out=x*self.ca(x)\n\u001b[1;32m    164\u001b[0m #         out=out*self.sa(out)\n\u001b[1;32m    165\u001b[0m #         return out+residual\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m # class h_sigmoid(nn.Module):\n\u001b[1;32m    168\u001b[0m #     def __init__(self, inplace=True):\n\u001b[1;32m    169\u001b[0m #         super(h_sigmoid, self).__init__()\n\u001b[1;32m    170\u001b[0m #         self.relu = nn.ReLU6(inplace=inplace)\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m #     def forward(self, x):\n\u001b[1;32m    173\u001b[0m #         return self.relu(x + 3) / 6\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \n\u001b[1;32m    176\u001b[0m # class h_swish(nn.Module):\n\u001b[1;32m    177\u001b[0m #     def __init__(self, inplace=True):\n\u001b[1;32m    178\u001b[0m #         super(h_swish, self).__init__()\n\u001b[1;32m    179\u001b[0m #         self.sigmoid = h_sigmoid(inplace=inplace)\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m #     def forward(self, x):\n\u001b[1;32m    182\u001b[0m #         return x * self.sigmoid(x)\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \n\u001b[1;32m    185\u001b[0m # class ECALayer(nn.Module):\n\u001b[1;32m    186\u001b[0m #     def __init__(self, channel, gamma=2, b=1, sigmoid=True):\n\u001b[1;32m    187\u001b[0m #         super(ECALayer, self).__init__()\n\u001b[1;32m    188\u001b[0m #         t = int(abs((math.log(channel, 2) + b) / gamma))\n\u001b[1;32m    189\u001b[0m #         k = t if t % 2 else t + 1\n\u001b[1;32m    190\u001b[0m \n\u001b[1;32m    191\u001b[0m #         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\u001b[1;32m    192\u001b[0m #         self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=k // 2, bias=False)\n\u001b[1;32m    193\u001b[0m #         if sigmoid:\n\u001b[1;32m    194\u001b[0m #             self.sigmoid = nn.Sigmoid()\n\u001b[1;32m    195\u001b[0m #         else:\n\u001b[1;32m    196\u001b[0m #             self.sigmoid = h_sigmoid()\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m #     def forward(self, x):\n\u001b[1;32m    199\u001b[0m #         y = self.avg_pool(x)\n\u001b[1;32m    200\u001b[0m #         y = self.conv(y.squeeze(-1).transpose(-1, -2))\n\u001b[1;32m    201\u001b[0m #         y = y.transpose(-1, -2).unsqueeze(-1)\n\u001b[1;32m    202\u001b[0m #         y = self.sigmoid(y)\n\u001b[1;32m    203\u001b[0m #         return x * y.expand_as(x)\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \n\u001b[1;32m    206\u001b[0m # class SELayer(nn.Module):\n\u001b[1;32m    207\u001b[0m #     def __init__(self, channel, reduction=4):\n\u001b[1;32m    208\u001b[0m #         super(SELayer, self).__init__()\n\u001b[1;32m    209\u001b[0m #         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\u001b[1;32m    210\u001b[0m #         self.fc = nn.Sequential(\n\u001b[1;32m    211\u001b[0m #                 nn.Linear(channel, channel // reduction),\n\u001b[1;32m    212\u001b[0m #                 nn.ReLU(inplace=True),\n\u001b[1;32m    213\u001b[0m #                 nn.Linear(channel // reduction, channel),\n\u001b[1;32m    214\u001b[0m #                 h_sigmoid()\n\u001b[1;32m    215\u001b[0m #         )\n\u001b[1;32m    216\u001b[0m \n\u001b[1;32m    217\u001b[0m #     def forward(self, x):\n\u001b[1;32m    218\u001b[0m #         b, c, _, _ = x.size()\n\u001b[1;32m    219\u001b[0m #         y = self.avg_pool(x).view(b, c)\n\u001b[1;32m    220\u001b[0m #         y = self.fc(y).view(b, c, 1, 1)\n\u001b[1;32m    221\u001b[0m #         return x * y\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m # class LocalityFeedForward(nn.Module):\n\u001b[1;32m    224\u001b[0m #     def __init__(self, in_dim, out_dim, stride, expand_ratio=4., act='hs+se', reduction=4,\n\u001b[1;32m    225\u001b[0m #                  wo_dp_conv=False, dp_first=False):\n\u001b[1;32m    226\u001b[0m #         \"\"\"\n\u001b[1;32m    227\u001b[0m #         :param in_dim: the input dimension\n\u001b[1;32m    228\u001b[0m #         :param out_dim: the output dimension. The input and output dimension should be the same.\n\u001b[1;32m    229\u001b[0m #         :param stride: stride of the depth-wise convolution.\n\u001b[1;32m    230\u001b[0m #         :param expand_ratio: expansion ratio of the hidden dimension.\n\u001b[1;32m    231\u001b[0m #         :param act: the activation function.\n\u001b[1;32m    232\u001b[0m #                     relu: ReLU\n\u001b[1;32m    233\u001b[0m #                     hs: h_swish\n\u001b[1;32m    234\u001b[0m #                     hs+se: h_swish and SE module\n\u001b[1;32m    235\u001b[0m #                     hs+eca: h_swish and ECA module\n\u001b[1;32m    236\u001b[0m #                     hs+ecah: h_swish and ECA module. Compared with eca, h_sigmoid is used.\n\u001b[1;32m    237\u001b[0m #         :param reduction: reduction rate in SE module.\n\u001b[1;32m    238\u001b[0m #         :param wo_dp_conv: without depth-wise convolution.\n\u001b[1;32m    239\u001b[0m #         :param dp_first: place depth-wise convolution as the first layer.\n\u001b[1;32m    240\u001b[0m #         \"\"\"\n\u001b[1;32m    241\u001b[0m #         super(LocalityFeedForward, self).__init__()\n\u001b[1;32m    242\u001b[0m #         hidden_dim = int(in_dim * expand_ratio)\n\u001b[1;32m    243\u001b[0m #         kernel_size = 3\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m #         layers = []\n\u001b[1;32m    246\u001b[0m #         # the first linear layer is replaced by 1x1 convolution.\n\u001b[1;32m    247\u001b[0m #         layers.extend([\n\u001b[1;32m    248\u001b[0m #             nn.Conv2d(in_dim, hidden_dim, 1, 1, 0, bias=False),\n\u001b[1;32m    249\u001b[0m #             nn.BatchNorm2d(hidden_dim),\n\u001b[1;32m    250\u001b[0m #             h_swish() if act.find('hs') >= 0 else nn.ReLU6(inplace=True)])\n\u001b[1;32m    251\u001b[0m \n\u001b[1;32m    252\u001b[0m #         # the depth-wise convolution between the two linear layers\n\u001b[1;32m    253\u001b[0m #         if not wo_dp_conv:\n\u001b[1;32m    254\u001b[0m #             dp = [\n\u001b[1;32m    255\u001b[0m #                 nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, kernel_size // 2, groups=hidden_dim, bias=False),\n\u001b[1;32m    256\u001b[0m #                 nn.BatchNorm2d(hidden_dim),\n\u001b[1;32m    257\u001b[0m #                 h_swish() if act.find('hs') >= 0 else nn.ReLU6(inplace=True)\n\u001b[1;32m    258\u001b[0m #             ]\n\u001b[1;32m    259\u001b[0m #             if dp_first:\n\u001b[1;32m    260\u001b[0m #                 layers = dp + layers\n\u001b[1;32m    261\u001b[0m #             else:\n\u001b[1;32m    262\u001b[0m #                 layers.extend(dp)\n\u001b[1;32m    263\u001b[0m \n\u001b[1;32m    264\u001b[0m #         if act.find('+') >= 0:\n\u001b[1;32m    265\u001b[0m #             attn = act.split('+')[1]\n\u001b[1;32m    266\u001b[0m #             if attn == 'se':\n\u001b[1;32m    267\u001b[0m #                 layers.append(SELayer(hidden_dim, reduction=reduction))\n\u001b[1;32m    268\u001b[0m #             elif attn.find('eca') >= 0:\n\u001b[1;32m    269\u001b[0m #                 layers.append(ECALayer(hidden_dim, sigmoid=attn == 'eca'))\n\u001b[1;32m    270\u001b[0m #             else:\n\u001b[1;32m    271\u001b[0m #                 raise NotImplementedError('Activation type {} is not implemented'.format(act))\n\u001b[1;32m    272\u001b[0m \n\u001b[1;32m    273\u001b[0m #         # the second linear layer is replaced by 1x1 convolution.\n\u001b[1;32m    274\u001b[0m #         layers.extend([\n\u001b[1;32m    275\u001b[0m #             nn.Conv2d(hidden_dim, out_dim, 1, 1, 0, bias=False),\n\u001b[1;32m    276\u001b[0m #             nn.BatchNorm2d(out_dim)\n\u001b[1;32m    277\u001b[0m #         ])\n\u001b[1;32m    278\u001b[0m #         self.conv = nn.Sequential(*layers)\n\u001b[1;32m    279\u001b[0m \n\u001b[1;32m    280\u001b[0m #     def forward(self, x):\n\u001b[1;32m    281\u001b[0m #         x = x + self.conv(x)\n\u001b[1;32m    282\u001b[0m #         return x\n\u001b[1;32m    283\u001b[0m \n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m # class Mlp(nn.Module):\n\u001b[1;32m    286\u001b[0m #     def __init__(self, in_features, out_features=None, mlp_ratio=None, drop=0., bias=True):\n\u001b[1;32m    287\u001b[0m #         super().__init__()\n\u001b[1;32m    288\u001b[0m #         out_features = out_features or in_features\n\u001b[1;32m    289\u001b[0m #         hidden_dim = _make_divisible(in_features * mlp_ratio, 32)\n\u001b[1;32m    290\u001b[0m #         self.conv1 = nn.Conv2d(in_features, hidden_dim, kernel_size=1, bias=bias)\n\u001b[1;32m    291\u001b[0m #         self.act = nn.ReLU(inplace=True)\n\u001b[1;32m    292\u001b[0m #         self.conv2 = nn.Conv2d(hidden_dim, out_features, kernel_size=1, bias=bias)\n\u001b[1;32m    293\u001b[0m #         self.drop = nn.Dropout(drop)\n\u001b[1;32m    294\u001b[0m \n\u001b[1;32m    295\u001b[0m #     def merge_bn(self, pre_norm):\n\u001b[1;32m    296\u001b[0m #         merge_pre_bn(self.conv1, pre_norm)\n\u001b[1;32m    297\u001b[0m \n\u001b[1;32m    298\u001b[0m #     def forward(self, x):\n\u001b[1;32m    299\u001b[0m #         x = self.conv1(x)\n\u001b[1;32m    300\u001b[0m #         x = self.act(x)\n\u001b[1;32m    301\u001b[0m #         x = self.drop(x)\n\u001b[1;32m    302\u001b[0m #         x = self.conv2(x)\n\u001b[1;32m    303\u001b[0m #         x = self.drop(x)\n\u001b[1;32m    304\u001b[0m #         return x\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \n\u001b[1;32m    307\u001b[0m # class ECB(nn.Module):\n\u001b[1;32m    308\u001b[0m #     \"\"\"\n\u001b[1;32m    309\u001b[0m #     Efficient Convolution Block\n\u001b[1;32m    310\u001b[0m #     \"\"\"\n\u001b[1;32m    311\u001b[0m #     def __init__(self, in_channels, out_channels, stride=1, path_dropout=0,\n\u001b[1;32m    312\u001b[0m #                  drop=0, head_dim=32, mlp_ratio=3):\n\u001b[1;32m    313\u001b[0m #         super(ECB, self).__init__()\n\u001b[1;32m    314\u001b[0m #         self.in_channels = in_channels\n\u001b[1;32m    315\u001b[0m #         self.out_channels = out_channels\n\u001b[1;32m    316\u001b[0m #         norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n\u001b[1;32m    317\u001b[0m #         assert out_channels % head_dim == 0\n\u001b[1;32m    318\u001b[0m \n\u001b[1;32m    319\u001b[0m #         self.patch_embed = PatchEmbed(in_channels, out_channels, stride)\n\u001b[1;32m    320\u001b[0m #         self.mhca = MHCA(out_channels, head_dim)\n\u001b[1;32m    321\u001b[0m #         self.attention_path_dropout = DropPath(path_dropout)\n\u001b[1;32m    322\u001b[0m \n\u001b[1;32m    323\u001b[0m #         self.conv = LocalityFeedForward(out_channels, out_channels, 1, mlp_ratio, reduction=out_channels)\n\u001b[1;32m    324\u001b[0m \n\u001b[1;32m    325\u001b[0m #         self.norm = norm_layer(out_channels)\n\u001b[1;32m    326\u001b[0m #         #self.mlp = Mlp(out_channels, mlp_ratio=mlp_ratio, drop=drop, bias=True)\n\u001b[1;32m    327\u001b[0m #         #self.mlp_path_dropout = DropPath(path_dropout)\n\u001b[1;32m    328\u001b[0m #         self.is_bn_merged = False\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m #     def merge_bn(self):\n\u001b[1;32m    331\u001b[0m #         if not self.is_bn_merged:\n\u001b[1;32m    332\u001b[0m #             self.mlp.merge_bn(self.norm)\n\u001b[1;32m    333\u001b[0m #             self.is_bn_merged = True\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m #     def forward(self, x):\n\u001b[1;32m    336\u001b[0m #         x = self.patch_embed(x)\n\u001b[1;32m    337\u001b[0m #         x = x + self.attention_path_dropout(self.mhca(x))\n\u001b[1;32m    338\u001b[0m #         if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n\u001b[1;32m    339\u001b[0m #             out = self.norm(x)\n\u001b[1;32m    340\u001b[0m #         else:\n\u001b[1;32m    341\u001b[0m #             out = x\n\u001b[1;32m    342\u001b[0m #         #x = x + self.mlp_path_dropout(self.mlp(out))\n\u001b[1;32m    343\u001b[0m #         x = x + self.conv(out) # (B, dim, 14, 14)\n\u001b[1;32m    344\u001b[0m #         return x\n\u001b[1;32m    345\u001b[0m \n\u001b[1;32m    346\u001b[0m \n\u001b[1;32m    347\u001b[0m # class E_MHSA(nn.Module):\n\u001b[1;32m    348\u001b[0m #     \"\"\"\n\u001b[1;32m    349\u001b[0m #     Efficient Multi-Head Self Attention\n\u001b[1;32m    350\u001b[0m #     \"\"\"\n\u001b[1;32m    351\u001b[0m     \n\u001b[1;32m    352\u001b[0m #     def __init__(self, dim, out_dim=None, head_dim=32, qkv_bias=True, qk_scale=None,\n\u001b[1;32m    353\u001b[0m #                  attn_drop=0, proj_drop=0., sr_ratio=1):\n\u001b[1;32m    354\u001b[0m #         super().__init__()\n\u001b[1;32m    355\u001b[0m #         self.dim = dim\n\u001b[1;32m    356\u001b[0m #         self.out_dim = out_dim if out_dim is not None else dim\n\u001b[1;32m    357\u001b[0m #         self.num_heads = self.dim // head_dim\n\u001b[1;32m    358\u001b[0m #         self.scale = qk_scale or head_dim ** -0.5\n\u001b[1;32m    359\u001b[0m #         self.q = nn.Linear(dim, self.dim, bias=qkv_bias)\n\u001b[1;32m    360\u001b[0m #         self.k = nn.Linear(dim, self.dim, bias=qkv_bias)\n\u001b[1;32m    361\u001b[0m #         self.v = nn.Linear(dim, self.dim, bias=qkv_bias)\n\u001b[1;32m    362\u001b[0m #         self.proj = nn.Linear(self.dim, self.out_dim)\n\u001b[1;32m    363\u001b[0m #         self.attn_drop = nn.Dropout(attn_drop)\n\u001b[1;32m    364\u001b[0m #         self.proj_drop = nn.Dropout(proj_drop)\n\u001b[1;32m    365\u001b[0m \n\u001b[1;32m    366\u001b[0m #         self.sr_ratio = sr_ratio\n\u001b[1;32m    367\u001b[0m #         self.N_ratio = sr_ratio ** 2\n\u001b[1;32m    368\u001b[0m #         if sr_ratio > 1:\n\u001b[1;32m    369\u001b[0m #             self.sr = nn.AvgPool1d(kernel_size=self.N_ratio, stride=self.N_ratio)\n\u001b[1;32m    370\u001b[0m #             self.norm = nn.BatchNorm1d(dim, eps=NORM_EPS)\n\u001b[1;32m    371\u001b[0m #         self.is_bn_merged = False\n\u001b[1;32m    372\u001b[0m \n\u001b[1;32m    373\u001b[0m #     def __init__(self, dim, out_dim=None, head_dim=32, qkv_bias=True, qk_scale=None,\n\u001b[1;32m    374\u001b[0m #                  attn_drop=0, proj_drop=0., sr_ratio=1):\n\u001b[1;32m    375\u001b[0m                  \n\u001b[1;32m    376\u001b[0m #         #self, d_model, , dropout=0.0):\n\u001b[1;32m    377\u001b[0m #         super().__init__()\n\u001b[1;32m    378\u001b[0m #         self.dim = dim\n\u001b[1;32m    379\u001b[0m #         self.out_dim = out_dim if out_dim is not None else dim\n\u001b[1;32m    380\u001b[0m #         self.num_heads = self.dim // head_dim\n\u001b[1;32m    381\u001b[0m #         self.scale = qk_scale or head_dim ** -0.5\n\u001b[1;32m    382\u001b[0m #         self.qkv = nn.Linear(dim, dim * 3)\n\u001b[1;32m    383\u001b[0m #         self.out = nn.Linear(dim, dim)\n\u001b[1;32m    384\u001b[0m #         self.dropout = nn.Dropout(attn_drop) \n\u001b[1;32m    385\u001b[0m #         self.is_bn_merged = False\n\u001b[1;32m    386\u001b[0m \n\u001b[1;32m    387\u001b[0m #     def forward(self, x):\n\u001b[1;32m    388\u001b[0m #         '''x: (B, T, D)'''\n\u001b[1;32m    389\u001b[0m #         q, k, v = self.qkv(x).chunk(3, dim=-1)\n\u001b[1;32m    390\u001b[0m #         q = q / q.norm(dim=-1, keepdim=True)\n\u001b[1;32m    391\u001b[0m #         k = k / k.norm(dim=-1, keepdim=True)\n\u001b[1;32m    392\u001b[0m         \n\u001b[1;32m    393\u001b[0m #         kvw = k * v\n\u001b[1;32m    394\u001b[0m #         if self.dropout.p > 0:\n\u001b[1;32m    395\u001b[0m #             kvw = self.dropout(kvw.transpose(-1, -2)).transpose(-1, -2) # dropout in seq dimension \n\u001b[1;32m    396\u001b[0m #         out = kvw.sum(dim=-2, keepdim=True) * q\n\u001b[1;32m    397\u001b[0m #         return self.out(out)\n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m     \n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \n\u001b[1;32m    402\u001b[0m #     def merge_bn(self, pre_bn):\n\u001b[1;32m    403\u001b[0m #         merge_pre_bn(self.q, pre_bn)\n\u001b[1;32m    404\u001b[0m #         if self.sr_ratio > 1:\n\u001b[1;32m    405\u001b[0m #             merge_pre_bn(self.k, pre_bn, self.norm)\n\u001b[1;32m    406\u001b[0m #             merge_pre_bn(self.v, pre_bn, self.norm)\n\u001b[1;32m    407\u001b[0m #         else:\n\u001b[1;32m    408\u001b[0m #             merge_pre_bn(self.k, pre_bn)\n\u001b[1;32m    409\u001b[0m #             merge_pre_bn(self.v, pre_bn)\n\u001b[1;32m    410\u001b[0m #         self.is_bn_merged = True\n\u001b[1;32m    411\u001b[0m \n\u001b[1;32m    412\u001b[0m \n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m # class LTB(nn.Module):\n\u001b[1;32m    416\u001b[0m #     \"\"\"\n\u001b[1;32m    417\u001b[0m #     Local Transformer Block\n\u001b[1;32m    418\u001b[0m #     \"\"\"\n\u001b[1;32m    419\u001b[0m #     def __init__(\n\u001b[1;32m    420\u001b[0m #             self, in_channels, out_channels, path_dropout, stride=1, sr_ratio=1,\n\u001b[1;32m    421\u001b[0m #             mlp_ratio=2, head_dim=32, mix_block_ratio=0.75, attn_drop=0, drop=0,\n\u001b[1;32m    422\u001b[0m #     ):\n\u001b[1;32m    423\u001b[0m #         super(LTB, self).__init__()\n\u001b[1;32m    424\u001b[0m #         self.in_channels = in_channels\n\u001b[1;32m    425\u001b[0m #         self.out_channels = out_channels\n\u001b[1;32m    426\u001b[0m #         self.mix_block_ratio = mix_block_ratio\n\u001b[1;32m    427\u001b[0m #         norm_func = partial(nn.BatchNorm2d, eps=NORM_EPS)\n\u001b[1;32m    428\u001b[0m \n\u001b[1;32m    429\u001b[0m #         self.mhsa_out_channels = _make_divisible(int(out_channels * mix_block_ratio), 32)\n\u001b[1;32m    430\u001b[0m #         self.mhca_out_channels = out_channels - self.mhsa_out_channels\n\u001b[1;32m    431\u001b[0m \n\u001b[1;32m    432\u001b[0m #         self.patch_embed = PatchEmbed(in_channels, self.mhsa_out_channels, stride)\n\u001b[1;32m    433\u001b[0m #         self.norm1 = norm_func(self.mhsa_out_channels)\n\u001b[1;32m    434\u001b[0m #         self.e_mhsa = E_MHSA(self.mhsa_out_channels, head_dim=head_dim, sr_ratio=sr_ratio,\n\u001b[1;32m    435\u001b[0m #                              attn_drop=attn_drop, proj_drop=drop)\n\u001b[1;32m    436\u001b[0m #         self.mhsa_path_dropout = DropPath(path_dropout * mix_block_ratio)\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m #         self.projection = PatchEmbed(self.mhsa_out_channels, self.mhca_out_channels, stride=1)\n\u001b[1;32m    439\u001b[0m #         self.mhca = MHCA(self.mhca_out_channels, head_dim=head_dim)\n\u001b[1;32m    440\u001b[0m #         self.mhca_path_dropout = DropPath(path_dropout * (1 - mix_block_ratio))\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m #         self.norm2 = norm_func(out_channels)\n\u001b[1;32m    443\u001b[0m #         self.conv = LocalityFeedForward(out_channels, out_channels, 1, mlp_ratio, reduction=out_channels)\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m #         #self.mlp = Mlp(out_channels, mlp_ratio=mlp_ratio, drop=drop)\n\u001b[1;32m    446\u001b[0m #         #self.mlp_path_dropout = DropPath(path_dropout)\n\u001b[1;32m    447\u001b[0m \n\u001b[1;32m    448\u001b[0m #         self.is_bn_merged = False\n\u001b[1;32m    449\u001b[0m \n\u001b[1;32m    450\u001b[0m #     def merge_bn(self):\n\u001b[1;32m    451\u001b[0m #         if not self.is_bn_merged:\n\u001b[1;32m    452\u001b[0m #             self.e_mhsa.merge_bn(self.norm1)\n\u001b[1;32m    453\u001b[0m #             self.mlp.merge_bn(self.norm2)\n\u001b[1;32m    454\u001b[0m #             self.is_bn_merged = True\n\u001b[1;32m    455\u001b[0m \n\u001b[1;32m    456\u001b[0m #     def forward(self, x):\n\u001b[1;32m    457\u001b[0m #         x = self.patch_embed(x)\n\u001b[1;32m    458\u001b[0m #         B, C, H, W = x.shape\n\u001b[1;32m    459\u001b[0m #         if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n\u001b[1;32m    460\u001b[0m #             out = self.norm1(x)\n\u001b[1;32m    461\u001b[0m #         else:\n\u001b[1;32m    462\u001b[0m #             out = x\n\u001b[1;32m    463\u001b[0m #         out = rearrange(out, \"b c h w -> b (h w) c\")  # b n c\n\u001b[1;32m    464\u001b[0m #         out = self.mhsa_path_dropout(self.e_mhsa(out))\n\u001b[1;32m    465\u001b[0m #         x = x + rearrange(out, \"b (h w) c -> b c h w\", h=H)\n\u001b[1;32m    466\u001b[0m \n\u001b[1;32m    467\u001b[0m #         out = self.projection(x)\n\u001b[1;32m    468\u001b[0m #         out = out + self.mhca_path_dropout(self.mhca(out))\n\u001b[1;32m    469\u001b[0m #         x = torch.cat([x, out], dim=1)\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m #         if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n\u001b[1;32m    472\u001b[0m #             out = self.norm2(x)\n\u001b[1;32m    473\u001b[0m #         else:\n\u001b[1;32m    474\u001b[0m #             out = x\n\u001b[1;32m    475\u001b[0m #         x = x + self.conv(out)\n\u001b[1;32m    476\u001b[0m #         #x = x + self.mlp_path_dropout(self.mlp(out))\n\u001b[1;32m    477\u001b[0m #         return x\n\u001b[1;32m    478\u001b[0m \n\u001b[1;32m    479\u001b[0m \n\u001b[1;32m    480\u001b[0m # class MedViT(nn.Module):\n\u001b[1;32m    481\u001b[0m #     def __init__(self, stem_chs, depths, path_dropout, attn_drop=0, drop=0, num_classes=1000,\n\u001b[1;32m    482\u001b[0m #                  strides=[1, 2, 2, 2], sr_ratios=[8, 4, 2, 1], head_dim=32, mix_block_ratio=0.75,\n\u001b[1;32m    483\u001b[0m #                  use_checkpoint=False):\n\u001b[1;32m    484\u001b[0m #         super(MedViT, self).__init__()\n\u001b[1;32m    485\u001b[0m #         self.use_checkpoint = use_checkpoint\n\u001b[1;32m    486\u001b[0m \n\u001b[1;32m    487\u001b[0m #         self.stage_out_channels = [[96] * (depths[0]),\n\u001b[1;32m    488\u001b[0m #                                    [192] * (depths[1] - 1) + [256],\n\u001b[1;32m    489\u001b[0m #                                    [384, 384, 384, 384, 512] * (depths[2] // 5),\n\u001b[1;32m    490\u001b[0m #                                    [768] * (depths[3] - 1) + [1024]]\n\u001b[1;32m    491\u001b[0m \n\u001b[1;32m    492\u001b[0m #         # Next Hybrid Strategy\n\u001b[1;32m    493\u001b[0m #         self.stage_block_types = [[ECB] * depths[0],\n\u001b[1;32m    494\u001b[0m #                                   [ECB] * (depths[1] - 1) + [LTB],\n\u001b[1;32m    495\u001b[0m #                                   [ECB, ECB, ECB, ECB, LTB] * (depths[2] // 5),\n\u001b[1;32m    496\u001b[0m #                                   [ECB] * (depths[3] - 1) + [LTB]]\n\u001b[1;32m    497\u001b[0m \n\u001b[1;32m    498\u001b[0m #         self.stem = nn.Sequential(\n\u001b[1;32m    499\u001b[0m #             ConvBNReLU(3, stem_chs[0], kernel_size=3, stride=2),\n\u001b[1;32m    500\u001b[0m #             ConvBNReLU(stem_chs[0], stem_chs[1], kernel_size=3, stride=1),\n\u001b[1;32m    501\u001b[0m #             ConvBNReLU(stem_chs[1], stem_chs[2], kernel_size=3, stride=1),\n\u001b[1;32m    502\u001b[0m #             ConvBNReLU(stem_chs[2], stem_chs[2], kernel_size=3, stride=2),\n\u001b[1;32m    503\u001b[0m #         )\n\u001b[1;32m    504\u001b[0m #         input_channel = stem_chs[-1]\n\u001b[1;32m    505\u001b[0m #         features = []\n\u001b[1;32m    506\u001b[0m #         idx = 0\n\u001b[1;32m    507\u001b[0m #         dpr = [x.item() for x in torch.linspace(0, path_dropout, sum(depths))]  # stochastic depth decay rule\n\u001b[1;32m    508\u001b[0m #         for stage_id in range(len(depths)):\n\u001b[1;32m    509\u001b[0m #             numrepeat = depths[stage_id]\n\u001b[1;32m    510\u001b[0m #             output_channels = self.stage_out_channels[stage_id]\n\u001b[1;32m    511\u001b[0m #             block_types = self.stage_block_types[stage_id]\n\u001b[1;32m    512\u001b[0m #             for block_id in range(numrepeat):\n\u001b[1;32m    513\u001b[0m #                 if strides[stage_id] == 2 and block_id == 0:\n\u001b[1;32m    514\u001b[0m #                     stride = 2\n\u001b[1;32m    515\u001b[0m #                 else:\n\u001b[1;32m    516\u001b[0m #                     stride = 1\n\u001b[1;32m    517\u001b[0m #                 output_channel = output_channels[block_id]\n\u001b[1;32m    518\u001b[0m #                 block_type = block_types[block_id]\n\u001b[1;32m    519\u001b[0m #                 if block_type is ECB:\n\u001b[1;32m    520\u001b[0m #                     layer = ECB(input_channel, output_channel, stride=stride, path_dropout=dpr[idx + block_id],\n\u001b[1;32m    521\u001b[0m #                                 drop=drop, head_dim=head_dim)\n\u001b[1;32m    522\u001b[0m #                     features.append(layer)\n\u001b[1;32m    523\u001b[0m #                 elif block_type is LTB:\n\u001b[1;32m    524\u001b[0m #                     layer = LTB(input_channel, output_channel, path_dropout=dpr[idx + block_id], stride=stride,\n\u001b[1;32m    525\u001b[0m #                                 sr_ratio=sr_ratios[stage_id], head_dim=head_dim, mix_block_ratio=mix_block_ratio,\n\u001b[1;32m    526\u001b[0m #                                 attn_drop=attn_drop, drop=drop)\n\u001b[1;32m    527\u001b[0m #                     features.append(layer)\n\u001b[1;32m    528\u001b[0m #                 input_channel = output_channel\n\u001b[1;32m    529\u001b[0m #             idx += numrepeat\n\u001b[1;32m    530\u001b[0m #         self.features = nn.Sequential(*features)\n\u001b[1;32m    531\u001b[0m \n\u001b[1;32m    532\u001b[0m #         self.norm = nn.BatchNorm2d(output_channel, eps=NORM_EPS)\n\u001b[1;32m    533\u001b[0m \n\u001b[1;32m    534\u001b[0m #         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\u001b[1;32m    535\u001b[0m #         self.proj_head = nn.Sequential(\n\u001b[1;32m    536\u001b[0m #             nn.Linear(output_channel, num_classes),\n\u001b[1;32m    537\u001b[0m #         )\n\u001b[1;32m    538\u001b[0m \n\u001b[1;32m    539\u001b[0m #         self.stage_out_idx = [sum(depths[:idx + 1]) - 1 for idx in range(len(depths))]\n\u001b[1;32m    540\u001b[0m #         print('initialize_weights...')\n\u001b[1;32m    541\u001b[0m #         self._initialize_weights()\n\u001b[1;32m    542\u001b[0m \n\u001b[1;32m    543\u001b[0m #     def merge_bn(self):\n\u001b[1;32m    544\u001b[0m #         self.eval()\n\u001b[1;32m    545\u001b[0m #         for idx, module in self.named_modules():\n\u001b[1;32m    546\u001b[0m #             if isinstance(module, ECB) or isinstance(module, LTB):\n\u001b[1;32m    547\u001b[0m #                 module.merge_bn()\n\u001b[1;32m    548\u001b[0m \n\u001b[1;32m    549\u001b[0m #     def _initialize_weights(self):\n\u001b[1;32m    550\u001b[0m #         for n, m in self.named_modules():\n\u001b[1;32m    551\u001b[0m #             if isinstance(m, (nn.BatchNorm2d, nn.GroupNorm, nn.LayerNorm, nn.BatchNorm1d)):\n\u001b[1;32m    552\u001b[0m #                 nn.init.constant_(m.weight, 1.0)\n\u001b[1;32m    553\u001b[0m #                 nn.init.constant_(m.bias, 0)\n\u001b[1;32m    554\u001b[0m #             elif isinstance(m, nn.Linear):\n\u001b[1;32m    555\u001b[0m #                 trunc_normal_(m.weight, std=.02)\n\u001b[1;32m    556\u001b[0m #                 if hasattr(m, 'bias') and m.bias is not None:\n\u001b[1;32m    557\u001b[0m #                     nn.init.constant_(m.bias, 0)\n\u001b[1;32m    558\u001b[0m #             elif isinstance(m, nn.Conv2d):\n\u001b[1;32m    559\u001b[0m #                 trunc_normal_(m.weight, std=.02)\n\u001b[1;32m    560\u001b[0m #                 if hasattr(m, 'bias') and m.bias is not None:\n\u001b[1;32m    561\u001b[0m #                     nn.init.constant_(m.bias, 0)\n\u001b[1;32m    562\u001b[0m \n\u001b[1;32m    563\u001b[0m #     def forward(self, x):\n\u001b[1;32m    564\u001b[0m #         x = self.stem(x)\n\u001b[1;32m    565\u001b[0m #         for idx, layer in enumerate(self.features):\n\u001b[1;32m    566\u001b[0m #             if self.use_checkpoint:\n\u001b[0;32m--> 567\u001b[0m #                 x = checkpoint.checkpoint(layer, x)\n\u001b[1;32m    568\u001b[0m #             else:\n\u001b[1;32m    569\u001b[0m #                 x = layer(x)\n\u001b[1;32m    570\u001b[0m #         x = self.norm(x)\n\u001b[1;32m    571\u001b[0m #         x = self.avgpool(x)\n\u001b[1;32m    572\u001b[0m #         x = torch.flatten(x, 1)\n\u001b[1;32m    573\u001b[0m #         x = self.proj_head(x)\n\u001b[1;32m    574\u001b[0m #         return x\n\u001b[1;32m    575\u001b[0m \n\u001b[1;32m    576\u001b[0m \n\u001b[1;32m    577\u001b[0m # @register_model\n\u001b[1;32m    578\u001b[0m # def MedViT_small(pretrained=False, pretrained_cfg=None, **kwargs):\n\u001b[1;32m    579\u001b[0m #     model = MedViT(stem_chs=[64, 32, 64], depths=[3, 4, 10, 3], path_dropout=0.1, **kwargs)\n\u001b[1;32m    580\u001b[0m #     return model\n\u001b[1;32m    581\u001b[0m \n\u001b[1;32m    582\u001b[0m \n\u001b[1;32m    583\u001b[0m # @register_model\n\u001b[1;32m    584\u001b[0m # def MedViT_base(pretrained=False, pretrained_cfg=None, **kwargs):\n\u001b[1;32m    585\u001b[0m #     model = MedViT(stem_chs=[64, 32, 64], depths=[3, 4, 20, 3], path_dropout=0.2, **kwargs)\n\u001b[1;32m    586\u001b[0m #     return model\n\u001b[1;32m    587\u001b[0m \n\u001b[1;32m    588\u001b[0m \n\u001b[1;32m    589\u001b[0m # @register_model\n\u001b[1;32m    590\u001b[0m # def MedViT_large(pretrained=False, pretrained_cfg=None, **kwargs):\n\u001b[1;32m    591\u001b[0m #     model = MedViT(stem_chs=[64, 32, 64], depths=[3, 4, 30, 3], path_dropout=0.2, **kwargs)\n\u001b[1;32m    592\u001b[0m #     return model\n\u001b[1;32m    593\u001b[0m \n\u001b[1;32m    594\u001b[0m \n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \n\u001b[1;32m    597\u001b[0m \n\u001b[1;32m    598\u001b[0m # QUACK\n\u001b[1;32m    599\u001b[0m \"\"\"\n\u001b[1;32m    600\u001b[0m Author: Omid Nejati\n\u001b[1;32m    601\u001b[0m Email: omid_nejaty@alumni.iust.ac.ir\n\u001b[1;32m    602\u001b[0m \n\u001b[1;32m    603\u001b[0m MedViT: A Robust Vision Transformer for Generalized Medical Image Classification.\n\u001b[1;32m    604\u001b[0m \"\"\"\n\u001b[1;32m    605\u001b[0m from functools import partial\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/computer_vision/cv-final-project/CVFinalProject/MedViT.py:343\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      1\u001b[0m # \"\"\"\n\u001b[1;32m      2\u001b[0m # Author: Omid Nejati\n\u001b[1;32m      3\u001b[0m # Email: omid_nejaty@alumni.iust.ac.ir\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m # MedViT: A Robust Vision Transformer for Generalized Medical Image Classification.\n\u001b[1;32m      6\u001b[0m # \"\"\"\n\u001b[1;32m      7\u001b[0m # from functools import partial\n\u001b[1;32m      8\u001b[0m # import math\n\u001b[1;32m      9\u001b[0m # import torch\n\u001b[1;32m     10\u001b[0m # import torch.utils.checkpoint as checkpoint\n\u001b[1;32m     11\u001b[0m # from einops import rearrange\n\u001b[1;32m     12\u001b[0m # from timm.models.layers import DropPath, trunc_normal_\n\u001b[1;32m     13\u001b[0m # from timm.models.registry import register_model\n\u001b[1;32m     14\u001b[0m # from torch import nn\n\u001b[1;32m     15\u001b[0m # from utils import merge_pre_bn\n\u001b[1;32m     16\u001b[0m # import numpy as np\n\u001b[1;32m     17\u001b[0m # import torch\n\u001b[1;32m     18\u001b[0m # from torch import nn\n\u001b[1;32m     19\u001b[0m # from torch.nn import init\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m # NORM_EPS = 1e-5\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m # class ConvBNReLU(nn.Module):\n\u001b[1;32m     25\u001b[0m #     def __init__(\n\u001b[1;32m     26\u001b[0m #             self,\n\u001b[1;32m     27\u001b[0m #             in_channels,\n\u001b[1;32m     28\u001b[0m #             out_channels,\n\u001b[1;32m     29\u001b[0m #             kernel_size,\n\u001b[1;32m     30\u001b[0m #             stride,\n\u001b[1;32m     31\u001b[0m #             groups=1):\n\u001b[1;32m     32\u001b[0m #         super(ConvBNReLU, self).__init__()\n\u001b[1;32m     33\u001b[0m #         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n\u001b[1;32m     34\u001b[0m #                               padding=1, groups=groups, bias=False)\n\u001b[1;32m     35\u001b[0m #         self.norm = nn.BatchNorm2d(out_channels, eps=NORM_EPS)\n\u001b[1;32m     36\u001b[0m #         self.act = nn.ReLU(inplace=True)\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m #     def forward(self, x):\n\u001b[1;32m     39\u001b[0m #         x = self.conv(x)\n\u001b[1;32m     40\u001b[0m #         x = self.norm(x)\n\u001b[1;32m     41\u001b[0m #         x = self.act(x)\n\u001b[1;32m     42\u001b[0m #         return x\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m # def _make_divisible(v, divisor, min_value=None):\n\u001b[1;32m     46\u001b[0m #     if min_value is None:\n\u001b[1;32m     47\u001b[0m #         min_value = divisor\n\u001b[1;32m     48\u001b[0m #     new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n\u001b[1;32m     49\u001b[0m #     # Make sure that round down does not go down by more than 10%.\n\u001b[1;32m     50\u001b[0m #     if new_v < 0.9 * v:\n\u001b[1;32m     51\u001b[0m #         new_v += divisor\n\u001b[1;32m     52\u001b[0m #     return new_v\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m # class PatchEmbed(nn.Module):\n\u001b[1;32m     56\u001b[0m #     def __init__(self,\n\u001b[1;32m     57\u001b[0m #                  in_channels,\n\u001b[1;32m     58\u001b[0m #                  out_channels,\n\u001b[1;32m     59\u001b[0m #                  stride=1):\n\u001b[1;32m     60\u001b[0m #         super(PatchEmbed, self).__init__()\n\u001b[1;32m     61\u001b[0m #         norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n\u001b[1;32m     62\u001b[0m #         if stride == 2:\n\u001b[1;32m     63\u001b[0m #             self.avgpool = nn.AvgPool2d((2, 2), stride=2, ceil_mode=True, count_include_pad=False)\n\u001b[1;32m     64\u001b[0m #             self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n\u001b[1;32m     65\u001b[0m #             self.norm = norm_layer(out_channels)\n\u001b[1;32m     66\u001b[0m #         elif in_channels != out_channels:\n\u001b[1;32m     67\u001b[0m #             self.avgpool = nn.Identity()\n\u001b[1;32m     68\u001b[0m #             self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n\u001b[1;32m     69\u001b[0m #             self.norm = norm_layer(out_channels)\n\u001b[1;32m     70\u001b[0m #         else:\n\u001b[1;32m     71\u001b[0m #             self.avgpool = nn.Identity()\n\u001b[1;32m     72\u001b[0m #             self.conv = nn.Identity()\n\u001b[1;32m     73\u001b[0m #             self.norm = nn.Identity()\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m #     def forward(self, x):\n\u001b[1;32m     76\u001b[0m #         return self.norm(self.conv(self.avgpool(x)))\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m # # class MHCA(nn.Module):\n\u001b[1;32m     80\u001b[0m # #     \"\"\"\n\u001b[1;32m     81\u001b[0m # #     Multi-Head Convolutional Attention\n\u001b[1;32m     82\u001b[0m # #     \"\"\"\n\u001b[1;32m     83\u001b[0m # #     def __init__(self, out_channels, head_dim):\n\u001b[1;32m     84\u001b[0m # #         super(MHCA, self).__init__()\n\u001b[1;32m     85\u001b[0m # #         norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n\u001b[1;32m     86\u001b[0m # #         self.group_conv3x3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1,\n\u001b[1;32m     87\u001b[0m # #                                        padding=1, groups=out_channels // head_dim, bias=False)\n\u001b[1;32m     88\u001b[0m # #         self.norm = norm_layer(out_channels)\n\u001b[1;32m     89\u001b[0m # #         self.act = nn.ReLU(inplace=True)\n\u001b[1;32m     90\u001b[0m # #         self.projection = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=False)\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m # #     def forward(self, x):\n\u001b[1;32m     93\u001b[0m # #         out = self.group_conv3x3(x)\n\u001b[1;32m     94\u001b[0m # #         out = self.norm(out)\n\u001b[1;32m     95\u001b[0m # #         out = self.act(out)\n\u001b[1;32m     96\u001b[0m # #         out = self.projection(out)\n\u001b[1;32m     97\u001b[0m # #         return out\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m # class ChannelAttention(nn.Module):\n\u001b[1;32m    100\u001b[0m #     def __init__(self,channel,reduction=16):\n\u001b[1;32m    101\u001b[0m #         super().__init__()\n\u001b[1;32m    102\u001b[0m #         self.maxpool=nn.AdaptiveMaxPool2d(1)\n\u001b[1;32m    103\u001b[0m #         self.avgpool=nn.AdaptiveAvgPool2d(1)\n\u001b[1;32m    104\u001b[0m #         self.se=nn.Sequential(\n\u001b[1;32m    105\u001b[0m #             nn.Conv2d(channel,channel//reduction,1,bias=False),\n\u001b[1;32m    106\u001b[0m #             nn.ReLU(),\n\u001b[1;32m    107\u001b[0m #             nn.Conv2d(channel//reduction,channel,1,bias=False)\n\u001b[1;32m    108\u001b[0m #         )\n\u001b[1;32m    109\u001b[0m #         self.sigmoid=nn.Sigmoid()\n\u001b[1;32m    110\u001b[0m     \n\u001b[1;32m    111\u001b[0m #     def forward(self, x) :\n\u001b[1;32m    112\u001b[0m #         max_result=self.maxpool(x)\n\u001b[1;32m    113\u001b[0m #         avg_result=self.avgpool(x)\n\u001b[1;32m    114\u001b[0m #         max_out=self.se(max_result)\n\u001b[1;32m    115\u001b[0m #         avg_out=self.se(avg_result)\n\u001b[1;32m    116\u001b[0m #         output=self.sigmoid(max_out+avg_out)\n\u001b[1;32m    117\u001b[0m #         return output\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m # class SpatialAttention(nn.Module):\n\u001b[1;32m    120\u001b[0m #     def __init__(self,kernel_size=7):\n\u001b[1;32m    121\u001b[0m #         super().__init__()\n\u001b[1;32m    122\u001b[0m #         self.conv=nn.Conv2d(2,1,kernel_size=kernel_size,padding=kernel_size//2)\n\u001b[1;32m    123\u001b[0m #         self.sigmoid=nn.Sigmoid()\n\u001b[1;32m    124\u001b[0m     \n\u001b[1;32m    125\u001b[0m #     def forward(self, x) :\n\u001b[1;32m    126\u001b[0m #         max_result,_=torch.max(x,dim=1,keepdim=True)\n\u001b[1;32m    127\u001b[0m #         avg_result=torch.mean(x,dim=1,keepdim=True)\n\u001b[1;32m    128\u001b[0m #         result=torch.cat([max_result,avg_result],1)\n\u001b[1;32m    129\u001b[0m #         output=self.conv(result)\n\u001b[1;32m    130\u001b[0m #         output=self.sigmoid(output)\n\u001b[1;32m    131\u001b[0m #         return output\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m # class MHCA(nn.Module):\n\u001b[1;32m    136\u001b[0m #     \"\"\"\n\u001b[1;32m    137\u001b[0m #     Convolutional Block Attention Module\n\u001b[1;32m    138\u001b[0m #     \"\"\"\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m #     def __init__(self, out_channels, head_dim, reduction=16,kernel_size=49):\n\u001b[1;32m    141\u001b[0m #         super().__init__()\n\u001b[1;32m    142\u001b[0m #         self.ca=ChannelAttention(channel=out_channels,reduction=reduction)\n\u001b[1;32m    143\u001b[0m #         self.sa=SpatialAttention(kernel_size=kernel_size)\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m #     def init_weights(self):\n\u001b[1;32m    147\u001b[0m #         for m in self.modules():\n\u001b[1;32m    148\u001b[0m #             if isinstance(m, nn.Conv2d):\n\u001b[1;32m    149\u001b[0m #                 init.kaiming_normal_(m.weight, mode='fan_out')\n\u001b[1;32m    150\u001b[0m #                 if m.bias is not None:\n\u001b[1;32m    151\u001b[0m #                     init.constant_(m.bias, 0)\n\u001b[1;32m    152\u001b[0m #             elif isinstance(m, nn.BatchNorm2d):\n\u001b[1;32m    153\u001b[0m #                 init.constant_(m.weight, 1)\n\u001b[1;32m    154\u001b[0m #                 init.constant_(m.bias, 0)\n\u001b[1;32m    155\u001b[0m #             elif isinstance(m, nn.Linear):\n\u001b[1;32m    156\u001b[0m #                 init.normal_(m.weight, std=0.001)\n\u001b[1;32m    157\u001b[0m #                 if m.bias is not None:\n\u001b[1;32m    158\u001b[0m #                     init.constant_(m.bias, 0)\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m #     def forward(self, x):\n\u001b[1;32m    161\u001b[0m #         b, c, _, _ = x.size()\n\u001b[1;32m    162\u001b[0m #         residual=x\n\u001b[1;32m    163\u001b[0m #         out=x*self.ca(x)\n\u001b[1;32m    164\u001b[0m #         out=out*self.sa(out)\n\u001b[1;32m    165\u001b[0m #         return out+residual\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m # class h_sigmoid(nn.Module):\n\u001b[1;32m    168\u001b[0m #     def __init__(self, inplace=True):\n\u001b[1;32m    169\u001b[0m #         super(h_sigmoid, self).__init__()\n\u001b[1;32m    170\u001b[0m #         self.relu = nn.ReLU6(inplace=inplace)\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m #     def forward(self, x):\n\u001b[1;32m    173\u001b[0m #         return self.relu(x + 3) / 6\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \n\u001b[1;32m    176\u001b[0m # class h_swish(nn.Module):\n\u001b[1;32m    177\u001b[0m #     def __init__(self, inplace=True):\n\u001b[1;32m    178\u001b[0m #         super(h_swish, self).__init__()\n\u001b[1;32m    179\u001b[0m #         self.sigmoid = h_sigmoid(inplace=inplace)\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m #     def forward(self, x):\n\u001b[1;32m    182\u001b[0m #         return x * self.sigmoid(x)\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \n\u001b[1;32m    185\u001b[0m # class ECALayer(nn.Module):\n\u001b[1;32m    186\u001b[0m #     def __init__(self, channel, gamma=2, b=1, sigmoid=True):\n\u001b[1;32m    187\u001b[0m #         super(ECALayer, self).__init__()\n\u001b[1;32m    188\u001b[0m #         t = int(abs((math.log(channel, 2) + b) / gamma))\n\u001b[1;32m    189\u001b[0m #         k = t if t % 2 else t + 1\n\u001b[1;32m    190\u001b[0m \n\u001b[1;32m    191\u001b[0m #         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\u001b[1;32m    192\u001b[0m #         self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=k // 2, bias=False)\n\u001b[1;32m    193\u001b[0m #         if sigmoid:\n\u001b[1;32m    194\u001b[0m #             self.sigmoid = nn.Sigmoid()\n\u001b[1;32m    195\u001b[0m #         else:\n\u001b[1;32m    196\u001b[0m #             self.sigmoid = h_sigmoid()\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m #     def forward(self, x):\n\u001b[1;32m    199\u001b[0m #         y = self.avg_pool(x)\n\u001b[1;32m    200\u001b[0m #         y = self.conv(y.squeeze(-1).transpose(-1, -2))\n\u001b[1;32m    201\u001b[0m #         y = y.transpose(-1, -2).unsqueeze(-1)\n\u001b[1;32m    202\u001b[0m #         y = self.sigmoid(y)\n\u001b[1;32m    203\u001b[0m #         return x * y.expand_as(x)\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \n\u001b[1;32m    206\u001b[0m # class SELayer(nn.Module):\n\u001b[1;32m    207\u001b[0m #     def __init__(self, channel, reduction=4):\n\u001b[1;32m    208\u001b[0m #         super(SELayer, self).__init__()\n\u001b[1;32m    209\u001b[0m #         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\u001b[1;32m    210\u001b[0m #         self.fc = nn.Sequential(\n\u001b[1;32m    211\u001b[0m #                 nn.Linear(channel, channel // reduction),\n\u001b[1;32m    212\u001b[0m #                 nn.ReLU(inplace=True),\n\u001b[1;32m    213\u001b[0m #                 nn.Linear(channel // reduction, channel),\n\u001b[1;32m    214\u001b[0m #                 h_sigmoid()\n\u001b[1;32m    215\u001b[0m #         )\n\u001b[1;32m    216\u001b[0m \n\u001b[1;32m    217\u001b[0m #     def forward(self, x):\n\u001b[1;32m    218\u001b[0m #         b, c, _, _ = x.size()\n\u001b[1;32m    219\u001b[0m #         y = self.avg_pool(x).view(b, c)\n\u001b[1;32m    220\u001b[0m #         y = self.fc(y).view(b, c, 1, 1)\n\u001b[1;32m    221\u001b[0m #         return x * y\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m # class LocalityFeedForward(nn.Module):\n\u001b[1;32m    224\u001b[0m #     def __init__(self, in_dim, out_dim, stride, expand_ratio=4., act='hs+se', reduction=4,\n\u001b[1;32m    225\u001b[0m #                  wo_dp_conv=False, dp_first=False):\n\u001b[1;32m    226\u001b[0m #         \"\"\"\n\u001b[1;32m    227\u001b[0m #         :param in_dim: the input dimension\n\u001b[1;32m    228\u001b[0m #         :param out_dim: the output dimension. The input and output dimension should be the same.\n\u001b[1;32m    229\u001b[0m #         :param stride: stride of the depth-wise convolution.\n\u001b[1;32m    230\u001b[0m #         :param expand_ratio: expansion ratio of the hidden dimension.\n\u001b[1;32m    231\u001b[0m #         :param act: the activation function.\n\u001b[1;32m    232\u001b[0m #                     relu: ReLU\n\u001b[1;32m    233\u001b[0m #                     hs: h_swish\n\u001b[1;32m    234\u001b[0m #                     hs+se: h_swish and SE module\n\u001b[1;32m    235\u001b[0m #                     hs+eca: h_swish and ECA module\n\u001b[1;32m    236\u001b[0m #                     hs+ecah: h_swish and ECA module. Compared with eca, h_sigmoid is used.\n\u001b[1;32m    237\u001b[0m #         :param reduction: reduction rate in SE module.\n\u001b[1;32m    238\u001b[0m #         :param wo_dp_conv: without depth-wise convolution.\n\u001b[1;32m    239\u001b[0m #         :param dp_first: place depth-wise convolution as the first layer.\n\u001b[1;32m    240\u001b[0m #         \"\"\"\n\u001b[1;32m    241\u001b[0m #         super(LocalityFeedForward, self).__init__()\n\u001b[1;32m    242\u001b[0m #         hidden_dim = int(in_dim * expand_ratio)\n\u001b[1;32m    243\u001b[0m #         kernel_size = 3\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m #         layers = []\n\u001b[1;32m    246\u001b[0m #         # the first linear layer is replaced by 1x1 convolution.\n\u001b[1;32m    247\u001b[0m #         layers.extend([\n\u001b[1;32m    248\u001b[0m #             nn.Conv2d(in_dim, hidden_dim, 1, 1, 0, bias=False),\n\u001b[1;32m    249\u001b[0m #             nn.BatchNorm2d(hidden_dim),\n\u001b[1;32m    250\u001b[0m #             h_swish() if act.find('hs') >= 0 else nn.ReLU6(inplace=True)])\n\u001b[1;32m    251\u001b[0m \n\u001b[1;32m    252\u001b[0m #         # the depth-wise convolution between the two linear layers\n\u001b[1;32m    253\u001b[0m #         if not wo_dp_conv:\n\u001b[1;32m    254\u001b[0m #             dp = [\n\u001b[1;32m    255\u001b[0m #                 nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, kernel_size // 2, groups=hidden_dim, bias=False),\n\u001b[1;32m    256\u001b[0m #                 nn.BatchNorm2d(hidden_dim),\n\u001b[1;32m    257\u001b[0m #                 h_swish() if act.find('hs') >= 0 else nn.ReLU6(inplace=True)\n\u001b[1;32m    258\u001b[0m #             ]\n\u001b[1;32m    259\u001b[0m #             if dp_first:\n\u001b[1;32m    260\u001b[0m #                 layers = dp + layers\n\u001b[1;32m    261\u001b[0m #             else:\n\u001b[1;32m    262\u001b[0m #                 layers.extend(dp)\n\u001b[1;32m    263\u001b[0m \n\u001b[1;32m    264\u001b[0m #         if act.find('+') >= 0:\n\u001b[1;32m    265\u001b[0m #             attn = act.split('+')[1]\n\u001b[1;32m    266\u001b[0m #             if attn == 'se':\n\u001b[1;32m    267\u001b[0m #                 layers.append(SELayer(hidden_dim, reduction=reduction))\n\u001b[1;32m    268\u001b[0m #             elif attn.find('eca') >= 0:\n\u001b[1;32m    269\u001b[0m #                 layers.append(ECALayer(hidden_dim, sigmoid=attn == 'eca'))\n\u001b[1;32m    270\u001b[0m #             else:\n\u001b[1;32m    271\u001b[0m #                 raise NotImplementedError('Activation type {} is not implemented'.format(act))\n\u001b[1;32m    272\u001b[0m \n\u001b[1;32m    273\u001b[0m #         # the second linear layer is replaced by 1x1 convolution.\n\u001b[1;32m    274\u001b[0m #         layers.extend([\n\u001b[1;32m    275\u001b[0m #             nn.Conv2d(hidden_dim, out_dim, 1, 1, 0, bias=False),\n\u001b[1;32m    276\u001b[0m #             nn.BatchNorm2d(out_dim)\n\u001b[1;32m    277\u001b[0m #         ])\n\u001b[1;32m    278\u001b[0m #         self.conv = nn.Sequential(*layers)\n\u001b[1;32m    279\u001b[0m \n\u001b[1;32m    280\u001b[0m #     def forward(self, x):\n\u001b[1;32m    281\u001b[0m #         x = x + self.conv(x)\n\u001b[1;32m    282\u001b[0m #         return x\n\u001b[1;32m    283\u001b[0m \n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m # class Mlp(nn.Module):\n\u001b[1;32m    286\u001b[0m #     def __init__(self, in_features, out_features=None, mlp_ratio=None, drop=0., bias=True):\n\u001b[1;32m    287\u001b[0m #         super().__init__()\n\u001b[1;32m    288\u001b[0m #         out_features = out_features or in_features\n\u001b[1;32m    289\u001b[0m #         hidden_dim = _make_divisible(in_features * mlp_ratio, 32)\n\u001b[1;32m    290\u001b[0m #         self.conv1 = nn.Conv2d(in_features, hidden_dim, kernel_size=1, bias=bias)\n\u001b[1;32m    291\u001b[0m #         self.act = nn.ReLU(inplace=True)\n\u001b[1;32m    292\u001b[0m #         self.conv2 = nn.Conv2d(hidden_dim, out_features, kernel_size=1, bias=bias)\n\u001b[1;32m    293\u001b[0m #         self.drop = nn.Dropout(drop)\n\u001b[1;32m    294\u001b[0m \n\u001b[1;32m    295\u001b[0m #     def merge_bn(self, pre_norm):\n\u001b[1;32m    296\u001b[0m #         merge_pre_bn(self.conv1, pre_norm)\n\u001b[1;32m    297\u001b[0m \n\u001b[1;32m    298\u001b[0m #     def forward(self, x):\n\u001b[1;32m    299\u001b[0m #         x = self.conv1(x)\n\u001b[1;32m    300\u001b[0m #         x = self.act(x)\n\u001b[1;32m    301\u001b[0m #         x = self.drop(x)\n\u001b[1;32m    302\u001b[0m #         x = self.conv2(x)\n\u001b[1;32m    303\u001b[0m #         x = self.drop(x)\n\u001b[1;32m    304\u001b[0m #         return x\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \n\u001b[1;32m    307\u001b[0m # class ECB(nn.Module):\n\u001b[1;32m    308\u001b[0m #     \"\"\"\n\u001b[1;32m    309\u001b[0m #     Efficient Convolution Block\n\u001b[1;32m    310\u001b[0m #     \"\"\"\n\u001b[1;32m    311\u001b[0m #     def __init__(self, in_channels, out_channels, stride=1, path_dropout=0,\n\u001b[1;32m    312\u001b[0m #                  drop=0, head_dim=32, mlp_ratio=3):\n\u001b[1;32m    313\u001b[0m #         super(ECB, self).__init__()\n\u001b[1;32m    314\u001b[0m #         self.in_channels = in_channels\n\u001b[1;32m    315\u001b[0m #         self.out_channels = out_channels\n\u001b[1;32m    316\u001b[0m #         norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n\u001b[1;32m    317\u001b[0m #         assert out_channels % head_dim == 0\n\u001b[1;32m    318\u001b[0m \n\u001b[1;32m    319\u001b[0m #         self.patch_embed = PatchEmbed(in_channels, out_channels, stride)\n\u001b[1;32m    320\u001b[0m #         self.mhca = MHCA(out_channels, head_dim)\n\u001b[1;32m    321\u001b[0m #         self.attention_path_dropout = DropPath(path_dropout)\n\u001b[1;32m    322\u001b[0m \n\u001b[1;32m    323\u001b[0m #         self.conv = LocalityFeedForward(out_channels, out_channels, 1, mlp_ratio, reduction=out_channels)\n\u001b[1;32m    324\u001b[0m \n\u001b[1;32m    325\u001b[0m #         self.norm = norm_layer(out_channels)\n\u001b[1;32m    326\u001b[0m #         #self.mlp = Mlp(out_channels, mlp_ratio=mlp_ratio, drop=drop, bias=True)\n\u001b[1;32m    327\u001b[0m #         #self.mlp_path_dropout = DropPath(path_dropout)\n\u001b[1;32m    328\u001b[0m #         self.is_bn_merged = False\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m #     def merge_bn(self):\n\u001b[1;32m    331\u001b[0m #         if not self.is_bn_merged:\n\u001b[1;32m    332\u001b[0m #             self.mlp.merge_bn(self.norm)\n\u001b[1;32m    333\u001b[0m #             self.is_bn_merged = True\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m #     def forward(self, x):\n\u001b[1;32m    336\u001b[0m #         x = self.patch_embed(x)\n\u001b[1;32m    337\u001b[0m #         x = x + self.attention_path_dropout(self.mhca(x))\n\u001b[1;32m    338\u001b[0m #         if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n\u001b[1;32m    339\u001b[0m #             out = self.norm(x)\n\u001b[1;32m    340\u001b[0m #         else:\n\u001b[1;32m    341\u001b[0m #             out = x\n\u001b[1;32m    342\u001b[0m #         #x = x + self.mlp_path_dropout(self.mlp(out))\n\u001b[0;32m--> 343\u001b[0m #         x = x + self.conv(out) # (B, dim, 14, 14)\n\u001b[1;32m    344\u001b[0m #         return x\n\u001b[1;32m    345\u001b[0m \n\u001b[1;32m    346\u001b[0m \n\u001b[1;32m    347\u001b[0m # class E_MHSA(nn.Module):\n\u001b[1;32m    348\u001b[0m #     \"\"\"\n\u001b[1;32m    349\u001b[0m #     Efficient Multi-Head Self Attention\n\u001b[1;32m    350\u001b[0m #     \"\"\"\n\u001b[1;32m    351\u001b[0m     \n\u001b[1;32m    352\u001b[0m #     def __init__(self, dim, out_dim=None, head_dim=32, qkv_bias=True, qk_scale=None,\n\u001b[1;32m    353\u001b[0m #                  attn_drop=0, proj_drop=0., sr_ratio=1):\n\u001b[1;32m    354\u001b[0m #         super().__init__()\n\u001b[1;32m    355\u001b[0m #         self.dim = dim\n\u001b[1;32m    356\u001b[0m #         self.out_dim = out_dim if out_dim is not None else dim\n\u001b[1;32m    357\u001b[0m #         self.num_heads = self.dim // head_dim\n\u001b[1;32m    358\u001b[0m #         self.scale = qk_scale or head_dim ** -0.5\n\u001b[1;32m    359\u001b[0m #         self.q = nn.Linear(dim, self.dim, bias=qkv_bias)\n\u001b[1;32m    360\u001b[0m #         self.k = nn.Linear(dim, self.dim, bias=qkv_bias)\n\u001b[1;32m    361\u001b[0m #         self.v = nn.Linear(dim, self.dim, bias=qkv_bias)\n\u001b[1;32m    362\u001b[0m #         self.proj = nn.Linear(self.dim, self.out_dim)\n\u001b[1;32m    363\u001b[0m #         self.attn_drop = nn.Dropout(attn_drop)\n\u001b[1;32m    364\u001b[0m #         self.proj_drop = nn.Dropout(proj_drop)\n\u001b[1;32m    365\u001b[0m \n\u001b[1;32m    366\u001b[0m #         self.sr_ratio = sr_ratio\n\u001b[1;32m    367\u001b[0m #         self.N_ratio = sr_ratio ** 2\n\u001b[1;32m    368\u001b[0m #         if sr_ratio > 1:\n\u001b[1;32m    369\u001b[0m #             self.sr = nn.AvgPool1d(kernel_size=self.N_ratio, stride=self.N_ratio)\n\u001b[1;32m    370\u001b[0m #             self.norm = nn.BatchNorm1d(dim, eps=NORM_EPS)\n\u001b[1;32m    371\u001b[0m #         self.is_bn_merged = False\n\u001b[1;32m    372\u001b[0m \n\u001b[1;32m    373\u001b[0m #     def __init__(self, dim, out_dim=None, head_dim=32, qkv_bias=True, qk_scale=None,\n\u001b[1;32m    374\u001b[0m #                  attn_drop=0, proj_drop=0., sr_ratio=1):\n\u001b[1;32m    375\u001b[0m                  \n\u001b[1;32m    376\u001b[0m #         #self, d_model, , dropout=0.0):\n\u001b[1;32m    377\u001b[0m #         super().__init__()\n\u001b[1;32m    378\u001b[0m #         self.dim = dim\n\u001b[1;32m    379\u001b[0m #         self.out_dim = out_dim if out_dim is not None else dim\n\u001b[1;32m    380\u001b[0m #         self.num_heads = self.dim // head_dim\n\u001b[1;32m    381\u001b[0m #         self.scale = qk_scale or head_dim ** -0.5\n\u001b[1;32m    382\u001b[0m #         self.qkv = nn.Linear(dim, dim * 3)\n\u001b[1;32m    383\u001b[0m #         self.out = nn.Linear(dim, dim)\n\u001b[1;32m    384\u001b[0m #         self.dropout = nn.Dropout(attn_drop) \n\u001b[1;32m    385\u001b[0m #         self.is_bn_merged = False\n\u001b[1;32m    386\u001b[0m \n\u001b[1;32m    387\u001b[0m #     def forward(self, x):\n\u001b[1;32m    388\u001b[0m #         '''x: (B, T, D)'''\n\u001b[1;32m    389\u001b[0m #         q, k, v = self.qkv(x).chunk(3, dim=-1)\n\u001b[1;32m    390\u001b[0m #         q = q / q.norm(dim=-1, keepdim=True)\n\u001b[1;32m    391\u001b[0m #         k = k / k.norm(dim=-1, keepdim=True)\n\u001b[1;32m    392\u001b[0m         \n\u001b[1;32m    393\u001b[0m #         kvw = k * v\n\u001b[1;32m    394\u001b[0m #         if self.dropout.p > 0:\n\u001b[1;32m    395\u001b[0m #             kvw = self.dropout(kvw.transpose(-1, -2)).transpose(-1, -2) # dropout in seq dimension \n\u001b[1;32m    396\u001b[0m #         out = kvw.sum(dim=-2, keepdim=True) * q\n\u001b[1;32m    397\u001b[0m #         return self.out(out)\n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m     \n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \n\u001b[1;32m    402\u001b[0m #     def merge_bn(self, pre_bn):\n\u001b[1;32m    403\u001b[0m #         merge_pre_bn(self.q, pre_bn)\n\u001b[1;32m    404\u001b[0m #         if self.sr_ratio > 1:\n\u001b[1;32m    405\u001b[0m #             merge_pre_bn(self.k, pre_bn, self.norm)\n\u001b[1;32m    406\u001b[0m #             merge_pre_bn(self.v, pre_bn, self.norm)\n\u001b[1;32m    407\u001b[0m #         else:\n\u001b[1;32m    408\u001b[0m #             merge_pre_bn(self.k, pre_bn)\n\u001b[1;32m    409\u001b[0m #             merge_pre_bn(self.v, pre_bn)\n\u001b[1;32m    410\u001b[0m #         self.is_bn_merged = True\n\u001b[1;32m    411\u001b[0m \n\u001b[1;32m    412\u001b[0m \n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m # class LTB(nn.Module):\n\u001b[1;32m    416\u001b[0m #     \"\"\"\n\u001b[1;32m    417\u001b[0m #     Local Transformer Block\n\u001b[1;32m    418\u001b[0m #     \"\"\"\n\u001b[1;32m    419\u001b[0m #     def __init__(\n\u001b[1;32m    420\u001b[0m #             self, in_channels, out_channels, path_dropout, stride=1, sr_ratio=1,\n\u001b[1;32m    421\u001b[0m #             mlp_ratio=2, head_dim=32, mix_block_ratio=0.75, attn_drop=0, drop=0,\n\u001b[1;32m    422\u001b[0m #     ):\n\u001b[1;32m    423\u001b[0m #         super(LTB, self).__init__()\n\u001b[1;32m    424\u001b[0m #         self.in_channels = in_channels\n\u001b[1;32m    425\u001b[0m #         self.out_channels = out_channels\n\u001b[1;32m    426\u001b[0m #         self.mix_block_ratio = mix_block_ratio\n\u001b[1;32m    427\u001b[0m #         norm_func = partial(nn.BatchNorm2d, eps=NORM_EPS)\n\u001b[1;32m    428\u001b[0m \n\u001b[1;32m    429\u001b[0m #         self.mhsa_out_channels = _make_divisible(int(out_channels * mix_block_ratio), 32)\n\u001b[1;32m    430\u001b[0m #         self.mhca_out_channels = out_channels - self.mhsa_out_channels\n\u001b[1;32m    431\u001b[0m \n\u001b[1;32m    432\u001b[0m #         self.patch_embed = PatchEmbed(in_channels, self.mhsa_out_channels, stride)\n\u001b[1;32m    433\u001b[0m #         self.norm1 = norm_func(self.mhsa_out_channels)\n\u001b[1;32m    434\u001b[0m #         self.e_mhsa = E_MHSA(self.mhsa_out_channels, head_dim=head_dim, sr_ratio=sr_ratio,\n\u001b[1;32m    435\u001b[0m #                              attn_drop=attn_drop, proj_drop=drop)\n\u001b[1;32m    436\u001b[0m #         self.mhsa_path_dropout = DropPath(path_dropout * mix_block_ratio)\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m #         self.projection = PatchEmbed(self.mhsa_out_channels, self.mhca_out_channels, stride=1)\n\u001b[1;32m    439\u001b[0m #         self.mhca = MHCA(self.mhca_out_channels, head_dim=head_dim)\n\u001b[1;32m    440\u001b[0m #         self.mhca_path_dropout = DropPath(path_dropout * (1 - mix_block_ratio))\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m #         self.norm2 = norm_func(out_channels)\n\u001b[1;32m    443\u001b[0m #         self.conv = LocalityFeedForward(out_channels, out_channels, 1, mlp_ratio, reduction=out_channels)\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m #         #self.mlp = Mlp(out_channels, mlp_ratio=mlp_ratio, drop=drop)\n\u001b[1;32m    446\u001b[0m #         #self.mlp_path_dropout = DropPath(path_dropout)\n\u001b[1;32m    447\u001b[0m \n\u001b[1;32m    448\u001b[0m #         self.is_bn_merged = False\n\u001b[1;32m    449\u001b[0m \n\u001b[1;32m    450\u001b[0m #     def merge_bn(self):\n\u001b[1;32m    451\u001b[0m #         if not self.is_bn_merged:\n\u001b[1;32m    452\u001b[0m #             self.e_mhsa.merge_bn(self.norm1)\n\u001b[1;32m    453\u001b[0m #             self.mlp.merge_bn(self.norm2)\n\u001b[1;32m    454\u001b[0m #             self.is_bn_merged = True\n\u001b[1;32m    455\u001b[0m \n\u001b[1;32m    456\u001b[0m #     def forward(self, x):\n\u001b[1;32m    457\u001b[0m #         x = self.patch_embed(x)\n\u001b[1;32m    458\u001b[0m #         B, C, H, W = x.shape\n\u001b[1;32m    459\u001b[0m #         if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n\u001b[1;32m    460\u001b[0m #             out = self.norm1(x)\n\u001b[1;32m    461\u001b[0m #         else:\n\u001b[1;32m    462\u001b[0m #             out = x\n\u001b[1;32m    463\u001b[0m #         out = rearrange(out, \"b c h w -> b (h w) c\")  # b n c\n\u001b[1;32m    464\u001b[0m #         out = self.mhsa_path_dropout(self.e_mhsa(out))\n\u001b[1;32m    465\u001b[0m #         x = x + rearrange(out, \"b (h w) c -> b c h w\", h=H)\n\u001b[1;32m    466\u001b[0m \n\u001b[1;32m    467\u001b[0m #         out = self.projection(x)\n\u001b[1;32m    468\u001b[0m #         out = out + self.mhca_path_dropout(self.mhca(out))\n\u001b[1;32m    469\u001b[0m #         x = torch.cat([x, out], dim=1)\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m #         if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n\u001b[1;32m    472\u001b[0m #             out = self.norm2(x)\n\u001b[1;32m    473\u001b[0m #         else:\n\u001b[1;32m    474\u001b[0m #             out = x\n\u001b[1;32m    475\u001b[0m #         x = x + self.conv(out)\n\u001b[1;32m    476\u001b[0m #         #x = x + self.mlp_path_dropout(self.mlp(out))\n\u001b[1;32m    477\u001b[0m #         return x\n\u001b[1;32m    478\u001b[0m \n\u001b[1;32m    479\u001b[0m \n\u001b[1;32m    480\u001b[0m # class MedViT(nn.Module):\n\u001b[1;32m    481\u001b[0m #     def __init__(self, stem_chs, depths, path_dropout, attn_drop=0, drop=0, num_classes=1000,\n\u001b[1;32m    482\u001b[0m #                  strides=[1, 2, 2, 2], sr_ratios=[8, 4, 2, 1], head_dim=32, mix_block_ratio=0.75,\n\u001b[1;32m    483\u001b[0m #                  use_checkpoint=False):\n\u001b[1;32m    484\u001b[0m #         super(MedViT, self).__init__()\n\u001b[1;32m    485\u001b[0m #         self.use_checkpoint = use_checkpoint\n\u001b[1;32m    486\u001b[0m \n\u001b[1;32m    487\u001b[0m #         self.stage_out_channels = [[96] * (depths[0]),\n\u001b[1;32m    488\u001b[0m #                                    [192] * (depths[1] - 1) + [256],\n\u001b[1;32m    489\u001b[0m #                                    [384, 384, 384, 384, 512] * (depths[2] // 5),\n\u001b[1;32m    490\u001b[0m #                                    [768] * (depths[3] - 1) + [1024]]\n\u001b[1;32m    491\u001b[0m \n\u001b[1;32m    492\u001b[0m #         # Next Hybrid Strategy\n\u001b[1;32m    493\u001b[0m #         self.stage_block_types = [[ECB] * depths[0],\n\u001b[1;32m    494\u001b[0m #                                   [ECB] * (depths[1] - 1) + [LTB],\n\u001b[1;32m    495\u001b[0m #                                   [ECB, ECB, ECB, ECB, LTB] * (depths[2] // 5),\n\u001b[1;32m    496\u001b[0m #                                   [ECB] * (depths[3] - 1) + [LTB]]\n\u001b[1;32m    497\u001b[0m \n\u001b[1;32m    498\u001b[0m #         self.stem = nn.Sequential(\n\u001b[1;32m    499\u001b[0m #             ConvBNReLU(3, stem_chs[0], kernel_size=3, stride=2),\n\u001b[1;32m    500\u001b[0m #             ConvBNReLU(stem_chs[0], stem_chs[1], kernel_size=3, stride=1),\n\u001b[1;32m    501\u001b[0m #             ConvBNReLU(stem_chs[1], stem_chs[2], kernel_size=3, stride=1),\n\u001b[1;32m    502\u001b[0m #             ConvBNReLU(stem_chs[2], stem_chs[2], kernel_size=3, stride=2),\n\u001b[1;32m    503\u001b[0m #         )\n\u001b[1;32m    504\u001b[0m #         input_channel = stem_chs[-1]\n\u001b[1;32m    505\u001b[0m #         features = []\n\u001b[1;32m    506\u001b[0m #         idx = 0\n\u001b[1;32m    507\u001b[0m #         dpr = [x.item() for x in torch.linspace(0, path_dropout, sum(depths))]  # stochastic depth decay rule\n\u001b[1;32m    508\u001b[0m #         for stage_id in range(len(depths)):\n\u001b[1;32m    509\u001b[0m #             numrepeat = depths[stage_id]\n\u001b[1;32m    510\u001b[0m #             output_channels = self.stage_out_channels[stage_id]\n\u001b[1;32m    511\u001b[0m #             block_types = self.stage_block_types[stage_id]\n\u001b[1;32m    512\u001b[0m #             for block_id in range(numrepeat):\n\u001b[1;32m    513\u001b[0m #                 if strides[stage_id] == 2 and block_id == 0:\n\u001b[1;32m    514\u001b[0m #                     stride = 2\n\u001b[1;32m    515\u001b[0m #                 else:\n\u001b[1;32m    516\u001b[0m #                     stride = 1\n\u001b[1;32m    517\u001b[0m #                 output_channel = output_channels[block_id]\n\u001b[1;32m    518\u001b[0m #                 block_type = block_types[block_id]\n\u001b[1;32m    519\u001b[0m #                 if block_type is ECB:\n\u001b[1;32m    520\u001b[0m #                     layer = ECB(input_channel, output_channel, stride=stride, path_dropout=dpr[idx + block_id],\n\u001b[1;32m    521\u001b[0m #                                 drop=drop, head_dim=head_dim)\n\u001b[1;32m    522\u001b[0m #                     features.append(layer)\n\u001b[1;32m    523\u001b[0m #                 elif block_type is LTB:\n\u001b[1;32m    524\u001b[0m #                     layer = LTB(input_channel, output_channel, path_dropout=dpr[idx + block_id], stride=stride,\n\u001b[1;32m    525\u001b[0m #                                 sr_ratio=sr_ratios[stage_id], head_dim=head_dim, mix_block_ratio=mix_block_ratio,\n\u001b[1;32m    526\u001b[0m #                                 attn_drop=attn_drop, drop=drop)\n\u001b[1;32m    527\u001b[0m #                     features.append(layer)\n\u001b[1;32m    528\u001b[0m #                 input_channel = output_channel\n\u001b[1;32m    529\u001b[0m #             idx += numrepeat\n\u001b[1;32m    530\u001b[0m #         self.features = nn.Sequential(*features)\n\u001b[1;32m    531\u001b[0m \n\u001b[1;32m    532\u001b[0m #         self.norm = nn.BatchNorm2d(output_channel, eps=NORM_EPS)\n\u001b[1;32m    533\u001b[0m \n\u001b[1;32m    534\u001b[0m #         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\u001b[1;32m    535\u001b[0m #         self.proj_head = nn.Sequential(\n\u001b[1;32m    536\u001b[0m #             nn.Linear(output_channel, num_classes),\n\u001b[1;32m    537\u001b[0m #         )\n\u001b[1;32m    538\u001b[0m \n\u001b[1;32m    539\u001b[0m #         self.stage_out_idx = [sum(depths[:idx + 1]) - 1 for idx in range(len(depths))]\n\u001b[1;32m    540\u001b[0m #         print('initialize_weights...')\n\u001b[1;32m    541\u001b[0m #         self._initialize_weights()\n\u001b[1;32m    542\u001b[0m \n\u001b[1;32m    543\u001b[0m #     def merge_bn(self):\n\u001b[1;32m    544\u001b[0m #         self.eval()\n\u001b[1;32m    545\u001b[0m #         for idx, module in self.named_modules():\n\u001b[1;32m    546\u001b[0m #             if isinstance(module, ECB) or isinstance(module, LTB):\n\u001b[1;32m    547\u001b[0m #                 module.merge_bn()\n\u001b[1;32m    548\u001b[0m \n\u001b[1;32m    549\u001b[0m #     def _initialize_weights(self):\n\u001b[1;32m    550\u001b[0m #         for n, m in self.named_modules():\n\u001b[1;32m    551\u001b[0m #             if isinstance(m, (nn.BatchNorm2d, nn.GroupNorm, nn.LayerNorm, nn.BatchNorm1d)):\n\u001b[1;32m    552\u001b[0m #                 nn.init.constant_(m.weight, 1.0)\n\u001b[1;32m    553\u001b[0m #                 nn.init.constant_(m.bias, 0)\n\u001b[1;32m    554\u001b[0m #             elif isinstance(m, nn.Linear):\n\u001b[1;32m    555\u001b[0m #                 trunc_normal_(m.weight, std=.02)\n\u001b[1;32m    556\u001b[0m #                 if hasattr(m, 'bias') and m.bias is not None:\n\u001b[1;32m    557\u001b[0m #                     nn.init.constant_(m.bias, 0)\n\u001b[1;32m    558\u001b[0m #             elif isinstance(m, nn.Conv2d):\n\u001b[1;32m    559\u001b[0m #                 trunc_normal_(m.weight, std=.02)\n\u001b[1;32m    560\u001b[0m #                 if hasattr(m, 'bias') and m.bias is not None:\n\u001b[1;32m    561\u001b[0m #                     nn.init.constant_(m.bias, 0)\n\u001b[1;32m    562\u001b[0m \n\u001b[1;32m    563\u001b[0m #     def forward(self, x):\n\u001b[1;32m    564\u001b[0m #         x = self.stem(x)\n\u001b[1;32m    565\u001b[0m #         for idx, layer in enumerate(self.features):\n\u001b[1;32m    566\u001b[0m #             if self.use_checkpoint:\n\u001b[1;32m    567\u001b[0m #                 x = checkpoint.checkpoint(layer, x)\n\u001b[1;32m    568\u001b[0m #             else:\n\u001b[1;32m    569\u001b[0m #                 x = layer(x)\n\u001b[1;32m    570\u001b[0m #         x = self.norm(x)\n\u001b[1;32m    571\u001b[0m #         x = self.avgpool(x)\n\u001b[1;32m    572\u001b[0m #         x = torch.flatten(x, 1)\n\u001b[1;32m    573\u001b[0m #         x = self.proj_head(x)\n\u001b[1;32m    574\u001b[0m #         return x\n\u001b[1;32m    575\u001b[0m \n\u001b[1;32m    576\u001b[0m \n\u001b[1;32m    577\u001b[0m # @register_model\n\u001b[1;32m    578\u001b[0m # def MedViT_small(pretrained=False, pretrained_cfg=None, **kwargs):\n\u001b[1;32m    579\u001b[0m #     model = MedViT(stem_chs=[64, 32, 64], depths=[3, 4, 10, 3], path_dropout=0.1, **kwargs)\n\u001b[1;32m    580\u001b[0m #     return model\n\u001b[1;32m    581\u001b[0m \n\u001b[1;32m    582\u001b[0m \n\u001b[1;32m    583\u001b[0m # @register_model\n\u001b[1;32m    584\u001b[0m # def MedViT_base(pretrained=False, pretrained_cfg=None, **kwargs):\n\u001b[1;32m    585\u001b[0m #     model = MedViT(stem_chs=[64, 32, 64], depths=[3, 4, 20, 3], path_dropout=0.2, **kwargs)\n\u001b[1;32m    586\u001b[0m #     return model\n\u001b[1;32m    587\u001b[0m \n\u001b[1;32m    588\u001b[0m \n\u001b[1;32m    589\u001b[0m # @register_model\n\u001b[1;32m    590\u001b[0m # def MedViT_large(pretrained=False, pretrained_cfg=None, **kwargs):\n\u001b[1;32m    591\u001b[0m #     model = MedViT(stem_chs=[64, 32, 64], depths=[3, 4, 30, 3], path_dropout=0.2, **kwargs)\n\u001b[1;32m    592\u001b[0m #     return model\n\u001b[1;32m    593\u001b[0m \n\u001b[1;32m    594\u001b[0m \n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \n\u001b[1;32m    597\u001b[0m \n\u001b[1;32m    598\u001b[0m # QUACK\n\u001b[1;32m    599\u001b[0m \"\"\"\n\u001b[1;32m    600\u001b[0m Author: Omid Nejati\n\u001b[1;32m    601\u001b[0m Email: omid_nejaty@alumni.iust.ac.ir\n\u001b[1;32m    602\u001b[0m \n\u001b[1;32m    603\u001b[0m MedViT: A Robust Vision Transformer for Generalized Medical Image Classification.\n\u001b[1;32m    604\u001b[0m \"\"\"\n\u001b[1;32m    605\u001b[0m from functools import partial\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    print('Epoch [%d/%d]'% (epoch+1, NUM_EPOCHS))\n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        inputs, targets = inputs.cpu(), targets.cpu()\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRkfM6CM91j8"
   },
   "source": [
    "##Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdC1Gg98RU37",
    "outputId": "11f6b052-2ac0-4080-c31d-cb5c4cdf0ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  auc: 0.716  acc: 0.522\n"
     ]
    }
   ],
   "source": [
    "split = 'test'\n",
    "\n",
    "model.eval()\n",
    "y_true = torch.tensor([])\n",
    "y_score = torch.tensor([])\n",
    "\n",
    "data_loader = train_loader_at_eval if split == 'train' else test_loader\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs = inputs.cpu()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.softmax(dim=-1)\n",
    "        y_score = torch.cat((y_score, outputs.cpu()), 0)\n",
    "\n",
    "    y_score = y_score.detach().numpy()\n",
    "\n",
    "    evaluator = Evaluator(data_flag, split, size=224)\n",
    "    metrics = evaluator.evaluate(y_score)\n",
    "\n",
    "    print('%s  auc: %.3f  acc: %.3f' % (split, *metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XH1WvchMX5L0"
   },
   "source": [
    "## Adversarial Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GER2-X0Yc1ZL"
   },
   "source": [
    "reduce bach size for GPU limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "rNHm98NaZjZ5"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TK4kV6p9YiZY"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "atk = FGSM(model, eps=0.01)\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    labels = labels.squeeze(1)\n",
    "    images = atk(images, labels).cuda()\n",
    "    outputs = model(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels.cuda()).sum()\n",
    "\n",
    "print('FGSM Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76g3n07lcW8x"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "atk = PGD(model, eps=8/255, alpha=4/255, steps=10, random_start=True)\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    labels = labels.squeeze(1)\n",
    "    images = atk(images, labels).cuda()\n",
    "    outputs = model(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels.cuda()).sum()\n",
    "\n",
    "print('PGD Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
